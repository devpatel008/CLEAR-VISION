{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb7cd0",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-20T17:32:48.970Z"
    },
    "papermill": {
     "duration": 0.003177,
     "end_time": "2025-05-20T17:37:47.914729",
     "exception": false,
     "start_time": "2025-05-20T17:37:47.911552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b864302",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-20T17:37:47.921936Z",
     "iopub.status.busy": "2025-05-20T17:37:47.921541Z",
     "iopub.status.idle": "2025-05-20T17:37:57.631738Z",
     "shell.execute_reply": "2025-05-20T17:37:57.630876Z"
    },
    "papermill": {
     "duration": 9.715769,
     "end_time": "2025-05-20T17:37:57.633039",
     "exception": false,
     "start_time": "2025-05-20T17:37:47.917270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# clear_vision_complete.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import grad\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from io import BytesIO\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Dataset and Data Loading Functions\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# Define transformations - using smaller images for CPU processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Custom dataset class for CelebA\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir)\n",
    "                          if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(root_dir='/kaggle/input/celebahq-resized-256x256/celeba_hq_256', max_images=None):\n",
    "    try:\n",
    "        dataset = CelebADataset(root_dir, transform=transform)\n",
    "        \n",
    "        # Limit dataset if specified\n",
    "        if max_images and max_images < len(dataset):\n",
    "            indices = list(range(len(dataset)))\n",
    "            random.shuffle(indices)\n",
    "            indices = indices[:max_images]\n",
    "            dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        \n",
    "        # Split dataset into train and validation sets\n",
    "        train_size = int(0.9 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create data loaders - small batch size for CPU\n",
    "        batch_size = 16\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(dataset)} images\")\n",
    "        print(f\"Training set: {len(train_dataset)} images\")\n",
    "        print(f\"Validation set: {len(val_dataset)} images\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Please ensure the CelebA dataset is correctly placed in the specified directory\")\n",
    "        return None, None\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Image Corruption Functions\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Mask-based corruption (as in the original repo)\n",
    "def apply_mask(images, mask_size=32, offset_x=None, offset_y=None):\n",
    "    \"\"\"Apply a square mask to the image at a specified or random position.\"\"\"\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    corrupted_images = images.clone()\n",
    "    \n",
    "    # For each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Determine mask position (random if not specified)\n",
    "        if offset_x is None:\n",
    "            offset_x = random.randint(0, width - mask_size)\n",
    "        if offset_y is None:\n",
    "            offset_y = random.randint(0, height - mask_size)\n",
    "        \n",
    "        # Apply mask (set to 0)\n",
    "        corrupted_images[i, :, offset_y:offset_y+mask_size, offset_x:offset_x+mask_size] = 0.0\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "# 2. Gaussian noise\n",
    "def apply_gaussian_noise(images, mean=0.0, std=0.1):\n",
    "    \"\"\"Add Gaussian noise to images.\"\"\"\n",
    "    noise = torch.randn_like(images) * std + mean\n",
    "    corrupted_images = images + noise\n",
    "    # Clip values to be in [-1, 1] range\n",
    "    corrupted_images = torch.clamp(corrupted_images, -1, 1)\n",
    "    return corrupted_images\n",
    "\n",
    "# 3. Salt and pepper noise\n",
    "def apply_salt_pepper_noise(images, salt_prob=0.02, pepper_prob=0.02):\n",
    "    \"\"\"Add salt and pepper noise to images.\"\"\"\n",
    "    corrupted_images = images.clone()\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    \n",
    "    # For each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Generate salt noise (white pixels)\n",
    "        salt_mask = torch.rand(channels, height, width, device=images.device) < salt_prob\n",
    "        corrupted_images[i][salt_mask] = 1.0\n",
    "        \n",
    "        # Generate pepper noise (black pixels)\n",
    "        pepper_mask = torch.rand(channels, height, width, device=images.device) < pepper_prob\n",
    "        corrupted_images[i][pepper_mask] = -1.0\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "# 4. Blur effects\n",
    "def apply_gaussian_blur(images, kernel_size=7, sigma=1.5):\n",
    "    \"\"\"Apply Gaussian blur to images.\"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    corrupted_images = images.clone()\n",
    "    \n",
    "    # Apply Gaussian blur to each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Make sure kernel size is odd\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        \n",
    "        for c in range(3): # Process each channel\n",
    "            img = images[i, c].cpu().numpy()\n",
    "            blurred = cv2.GaussianBlur(img, (kernel_size, kernel_size), sigma)\n",
    "            corrupted_images[i, c] = torch.from_numpy(blurred).to(images.device)\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "# 5. JPEG compression artifacts\n",
    "def apply_jpeg_artifacts(images, quality=10):\n",
    "    \"\"\"Apply JPEG compression artifacts to images.\"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    corrupted_images = torch.zeros_like(images)\n",
    "    \n",
    "    # Apply JPEG compression to each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Convert to PIL Image\n",
    "        img = transforms.ToPILImage()((images[i] + 1) / 2) # Denormalize\n",
    "        \n",
    "        # Save as JPEG and load back to apply compression artifacts\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\", quality=quality)\n",
    "        buffer.seek(0)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        \n",
    "        # Convert back to tensor and normalize\n",
    "        compressed_tensor = transforms.ToTensor()(compressed_img) * 2 - 1\n",
    "        corrupted_images[i] = compressed_tensor\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "# 6. Combined corruption (apply multiple types randomly)\n",
    "def apply_combined_corruption(images):\n",
    "    \"\"\"Apply a random combination of corruption types.\"\"\"\n",
    "    corrupted_images = images.clone()\n",
    "    \n",
    "    # List of corruption functions\n",
    "    corruptions = [\n",
    "        lambda x: apply_mask(x, mask_size=random.randint(20, 40)),\n",
    "        lambda x: apply_gaussian_noise(x, std=random.uniform(0.05, 0.2)),\n",
    "        lambda x: apply_salt_pepper_noise(x, salt_prob=random.uniform(0.01, 0.05),\n",
    "                                       pepper_prob=random.uniform(0.01, 0.05)),\n",
    "        lambda x: apply_gaussian_blur(x, kernel_size=random.choice([3, 5, 7]),\n",
    "                                    sigma=random.uniform(0.5, 2.0))\n",
    "    ]\n",
    "    \n",
    "    # Apply 1-3 random corruptions\n",
    "    num_corruptions = random.randint(1, 3)\n",
    "    selected_corruptions = random.sample(corruptions, num_corruptions)\n",
    "    \n",
    "    for corruption_fn in selected_corruptions:\n",
    "        corrupted_images = corruption_fn(corrupted_images)\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Model Architecture\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for better gradient flow and feature learning.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection if dimensions change\n",
    "        self.skip = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.silu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(residual)\n",
    "        out = F.silu(out)\n",
    "        return out\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(32, 32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(64, 64),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(128, 128),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(256, 256)\n",
    "        )\n",
    "        \n",
    "        # For 128x128 input, feature map size is now 8x8\n",
    "        self.encoder_output_size = 8\n",
    "        encoder_flatten_size = 256 * self.encoder_output_size * self.encoder_output_size\n",
    "        \n",
    "        # Latent space mapping\n",
    "        self.fc_mu = nn.Linear(encoder_flatten_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(encoder_flatten_size, latent_dim)\n",
    "        self.fc_decoder = nn.Linear(latent_dim, encoder_flatten_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            ResidualBlock(256, 256),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(128, 128),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(64, 64),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            ResidualBlock(32, 32),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.fc_decoder(z)\n",
    "        x = x.view(x.size(0), 256, self.encoder_output_size, self.encoder_output_size)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Loss Function and Training Utilities\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# Enhanced VAE loss function\n",
    "def vae_loss(reconstructed, original, mu, logvar, kld_weight=0.001):\n",
    "    \"\"\"\n",
    "    VAE loss with balanced reconstruction and KL divergence terms.\n",
    "    Lower KLD weight for better reconstructions.\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (MSE works better for image quality)\n",
    "    recon_loss = F.mse_loss(reconstructed, original, reduction='sum') / original.size(0)\n",
    "    \n",
    "    # KL Divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / original.size(0)\n",
    "    \n",
    "    # Total loss\n",
    "    loss = recon_loss + kld_weight * kld_loss\n",
    "    \n",
    "    return loss, recon_loss, kld_loss\n",
    "\n",
    "# Function to save sample images during training\n",
    "def save_sample_images(model, data_loader, epoch, output_dir='/kaggle/working/samples'):\n",
    "    \"\"\"Save sample reconstructions during training.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of images\n",
    "    batch = next(iter(data_loader))\n",
    "    if isinstance(batch, list):\n",
    "        batch = batch[0]  # For ImageFolder dataset\n",
    "    original_images = batch.to(device)[:8]  # Use 8 images\n",
    "    \n",
    "    # Apply different corruptions\n",
    "    mask_images = apply_mask(original_images.clone())\n",
    "    gaussian_images = apply_gaussian_noise(original_images.clone())\n",
    "    salt_pepper_images = apply_salt_pepper_noise(original_images.clone())\n",
    "    blur_images = apply_gaussian_blur(original_images.clone())\n",
    "    jpeg_images = apply_jpeg_artifacts(original_images.clone())\n",
    "    combined_images = apply_combined_corruption(original_images.clone())\n",
    "    \n",
    "    # Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        mask_reconstructed, _, _ = model(mask_images)\n",
    "        gaussian_reconstructed, _, _ = model(gaussian_images)\n",
    "        salt_pepper_reconstructed, _, _ = model(salt_pepper_images)\n",
    "        blur_reconstructed, _, _ = model(blur_images)\n",
    "        jpeg_reconstructed, _, _ = model(jpeg_images)\n",
    "        combined_reconstructed, _, _ = model(combined_images)\n",
    "    \n",
    "    # Create comparison grid: original, corrupted, reconstructed\n",
    "    rows = [\n",
    "        torch.cat([original_images, mask_images, mask_reconstructed], dim=0),\n",
    "        torch.cat([original_images, gaussian_images, gaussian_reconstructed], dim=0),\n",
    "        torch.cat([original_images, salt_pepper_images, salt_pepper_reconstructed], dim=0),\n",
    "        torch.cat([original_images, blur_images, blur_reconstructed], dim=0),\n",
    "        torch.cat([original_images, jpeg_images, jpeg_reconstructed], dim=0),\n",
    "        torch.cat([original_images, combined_images, combined_reconstructed], dim=0)\n",
    "    ]\n",
    "    \n",
    "    # Save grid\n",
    "    comparison = torch.cat(rows, dim=0)\n",
    "    save_image(comparison.cpu() * 0.5 + 0.5, f\"{output_dir}/epoch_{epoch}.png\",\n",
    "              nrow=8, padding=2, normalize=False)\n",
    "    \n",
    "    print(f\"Sample images saved for epoch {epoch}\")\n",
    "    model.train()\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# New LPIPS Implementation\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "class LPIPSMetric:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        # Try to load LPIPS package\n",
    "        try:\n",
    "            import lpips\n",
    "            self.lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "            self.has_lpips = True\n",
    "            print(\"LPIPS module loaded successfully\")\n",
    "        except ImportError:\n",
    "            print(\"Warning: LPIPS package not found. Using AlexNet features as approximation.\")\n",
    "            print(\"For better results, install LPIPS with: pip install lpips\")\n",
    "            # Use AlexNet features as approximation for LPIPS\n",
    "            self.has_lpips = False\n",
    "            self.alexnet = models.alexnet(pretrained=True).features[:9].eval().to(device)\n",
    "            for param in self.alexnet.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def calculate_lpips(self, img1, img2):\n",
    "        \"\"\"Calculate LPIPS between two images\"\"\"\n",
    "        # Ensure images have the same dimensions\n",
    "        if img1.shape[2:] != img2.shape[2:]:\n",
    "            img1 = F.interpolate(img1, size=img2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        if self.has_lpips:\n",
    "            # Use official LPIPS if available\n",
    "            with torch.no_grad():\n",
    "                lpips_dist = self.lpips_model(img1, img2)\n",
    "                return lpips_dist.mean().item()\n",
    "        else:\n",
    "            # Use AlexNet feature distance as approximation\n",
    "            with torch.no_grad():\n",
    "                # Make sure images are in range [-1, 1]\n",
    "                if img1.min() < -1 or img1.max() > 1:\n",
    "                    img1 = torch.clamp(img1, -1, 1)\n",
    "                if img2.min() < -1 or img2.max() > 1:\n",
    "                    img2 = torch.clamp(img2, -1, 1)\n",
    "                \n",
    "                # Convert from [-1,1] to [0,1] for AlexNet\n",
    "                img1 = (img1 + 1) / 2\n",
    "                img2 = (img2 + 1) / 2\n",
    "                \n",
    "                # Extract features\n",
    "                feat1 = self.alexnet(img1)\n",
    "                feat2 = self.alexnet(img2)\n",
    "                \n",
    "                # Calculate distance\n",
    "                lpips_dist = torch.mean((feat1 - feat2).pow(2))\n",
    "                return lpips_dist.item()\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# New FID Score Implementation\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "class FIDMetric:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        \n",
    "        # Load Inception v3 model for FID computation\n",
    "        try:\n",
    "            self.inception_model = models.inception_v3(pretrained=True, transform_input=False)\n",
    "            self.inception_model.fc = nn.Identity()  # Remove classification layer\n",
    "            self.inception_model.eval().to(device)\n",
    "            for param in self.inception_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.has_inception = True\n",
    "            print(\"Inception model loaded for FID calculation\")\n",
    "        except:\n",
    "            print(\"Warning: Could not load Inception model. FID scores will not be accurate.\")\n",
    "            try:\n",
    "                # Try with weights parameter instead\n",
    "                self.inception_model = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
    "                self.inception_model.fc = nn.Identity()  # Remove classification layer\n",
    "                self.inception_model.eval().to(device)\n",
    "                for param in self.inception_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.has_inception = True\n",
    "                print(\"Inception model loaded with updated API\")\n",
    "            except:\n",
    "                self.has_inception = False\n",
    "    \n",
    "    def get_activations(self, images, batch_size=50):\n",
    "        \"\"\"Get Inception activations for a batch of images\"\"\"\n",
    "        n_batches = len(images) // batch_size + 1\n",
    "        act = np.empty((len(images), 2048))\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(images))\n",
    "            \n",
    "            if start_idx >= end_idx:\n",
    "                break\n",
    "                \n",
    "            batch = images[start_idx:end_idx].to(self.device)\n",
    "            \n",
    "            # Resize to inception input size\n",
    "            if batch.shape[2] != 299 or batch.shape[3] != 299:\n",
    "                batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Normalize from [-1, 1] to [0, 1] to [0, 255]\n",
    "            if batch.min() < 0:\n",
    "                batch = (batch + 1) / 2  # [-1,1] -> [0,1]\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                pred = self.inception_model(batch)\n",
    "            \n",
    "            # Store activations\n",
    "            act[start_idx:end_idx] = pred.cpu().numpy()\n",
    "        \n",
    "        return act\n",
    "    \n",
    "    def calculate_activation_statistics(self, images):\n",
    "        \"\"\"Calculate statistics of activations for FID\"\"\"\n",
    "        activations = self.get_activations(images)\n",
    "        mu = np.mean(activations, axis=0)\n",
    "        sigma = np.cov(activations, rowvar=False)\n",
    "        return mu, sigma\n",
    "    \n",
    "    def calculate_frechet_distance(self, mu1, sigma1, mu2, sigma2):\n",
    "        \"\"\"Calculate Fréchet distance between two multivariate Gaussians\"\"\"\n",
    "        diff = mu1 - mu2\n",
    "        \n",
    "        # Product might be almost singular\n",
    "        covmean, _ = scipy.linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "        if not np.isfinite(covmean).all():\n",
    "            offset = np.eye(sigma1.shape[0]) * 1e-6\n",
    "            covmean = scipy.linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        \n",
    "        # Numerical error might give slight imaginary component\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        \n",
    "        tr_covmean = np.trace(covmean)\n",
    "        \n",
    "        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    \n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        \"\"\"Calculate FID between real and generated images\"\"\"\n",
    "        if not self.has_inception:\n",
    "            return -1  # Cannot calculate FID without inception model\n",
    "        \n",
    "        # Get statistics for real and generated images\n",
    "        mu_real, sigma_real = self.calculate_activation_statistics(real_images)\n",
    "        mu_gen, sigma_gen = self.calculate_activation_statistics(generated_images)\n",
    "        \n",
    "        # Calculate FID\n",
    "        fid_score = self.calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n",
    "        return fid_score\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Evaluation Metrics\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# Basic evaluation metrics\n",
    "def compute_psnr(img1, img2):\n",
    "    \"\"\"Compute Peak Signal-to-Noise Ratio between image tensors.\"\"\"\n",
    "    # Convert to range [0, 1] if in [-1, 1]\n",
    "    if img1.min() < 0:\n",
    "        img1 = img1 * 0.5 + 0.5\n",
    "    if img2.min() < 0:\n",
    "        img2 = img2 * 0.5 + 0.5\n",
    "    \n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "def compute_ssim(img1, img2):\n",
    "    \"\"\"Compute Structural Similarity Index between image tensors.\"\"\"\n",
    "    # Convert to range [0, 1] if in [-1, 1]\n",
    "    if img1.min() < 0:\n",
    "        img1 = img1 * 0.5 + 0.5\n",
    "    if img2.min() < 0:\n",
    "        img2 = img2 * 0.5 + 0.5\n",
    "    \n",
    "    # Constants\n",
    "    C1 = (0.01 * 1) ** 2\n",
    "    C2 = (0.03 * 1) ** 2\n",
    "    \n",
    "    # Calculate mean, variance, covariance\n",
    "    mu1 = torch.mean(img1, dim=(2, 3), keepdim=True)\n",
    "    mu2 = torch.mean(img2, dim=(2, 3), keepdim=True)\n",
    "    \n",
    "    sigma1_sq = torch.mean((img1 - mu1) ** 2, dim=(2, 3), keepdim=True)\n",
    "    sigma2_sq = torch.mean((img2 - mu2) ** 2, dim=(2, 3), keepdim=True)\n",
    "    sigma12 = torch.mean((img1 - mu1) * (img2 - mu2), dim=(2, 3), keepdim=True)\n",
    "    \n",
    "    # SSIM formula\n",
    "    numerator = (2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)\n",
    "    denominator = (mu1**2 + mu2**2 + C1) * (sigma1_sq + sigma2_sq + C2)\n",
    "    ssim = torch.mean(numerator / denominator)\n",
    "    \n",
    "    return ssim.item()\n",
    "\n",
    "# Comprehensive evaluation with all metrics\n",
    "def evaluate_model(model, data_loader, corruption_types=['mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined']):\n",
    "    \"\"\"Evaluate model on multiple corruption types with PSNR, SSIM, LPIPS and FID.\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize metrics\n",
    "    lpips_metric = LPIPSMetric(device)\n",
    "    fid_metric = FIDMetric(device)\n",
    "    \n",
    "    # Get a batch of images\n",
    "    batch = next(iter(data_loader))\n",
    "    if isinstance(batch, list):\n",
    "        batch = batch[0]\n",
    "    original_images = batch.to(device)[:32]  # Use 32 images for evaluation\n",
    "    \n",
    "    corruption_functions = {\n",
    "        'mask': apply_mask,\n",
    "        'gaussian': apply_gaussian_noise,\n",
    "        'salt_pepper': apply_salt_pepper_noise,\n",
    "        'blur': apply_gaussian_blur,\n",
    "        'jpeg': apply_jpeg_artifacts,\n",
    "        'combined': apply_combined_corruption\n",
    "    }\n",
    "    \n",
    "    # Create progress bar for corruption types evaluation\n",
    "    corruption_bar = tqdm(corruption_types, desc=\"Evaluating corruption types\")\n",
    "    \n",
    "    for corruption_type in corruption_bar:\n",
    "        corruption_bar.set_description(f\"Evaluating {corruption_type}\")\n",
    "        \n",
    "        if corruption_type not in corruption_functions:\n",
    "            print(f\"Unknown corruption type: {corruption_type}\")\n",
    "            continue\n",
    "        \n",
    "        # Apply corruption\n",
    "        corruption_fn = corruption_functions[corruption_type]\n",
    "        corrupted_images = corruption_fn(original_images.clone())\n",
    "        \n",
    "        # Get reconstructions\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            reconstructed_images, _, _ = model(corrupted_images)\n",
    "            inference_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics - silently, without progress bar\n",
    "        avg_psnr = 0\n",
    "        avg_ssim = 0\n",
    "        lpips_scores = []\n",
    "        \n",
    "        # Use regular loop instead of tqdm progress bar to avoid per-percentage updates\n",
    "        for i in range(len(original_images)):\n",
    "            psnr = compute_psnr(original_images[i:i+1], reconstructed_images[i:i+1])\n",
    "            ssim = compute_ssim(original_images[i:i+1], reconstructed_images[i:i+1])\n",
    "            lpips = lpips_metric.calculate_lpips(original_images[i:i+1], reconstructed_images[i:i+1])\n",
    "            \n",
    "            avg_psnr += psnr\n",
    "            avg_ssim += ssim\n",
    "            lpips_scores.append(lpips)\n",
    "        \n",
    "        avg_psnr /= len(original_images)\n",
    "        avg_ssim /= len(original_images)\n",
    "        avg_lpips = sum(lpips_scores) / len(lpips_scores)\n",
    "        \n",
    "        # Calculate FID (this is computationally expensive)\n",
    "        try:\n",
    "            fid_score = fid_metric.calculate_fid(original_images, reconstructed_images)\n",
    "        except:\n",
    "            fid_score = -1\n",
    "            print(f\"Warning: Failed to calculate FID score for {corruption_type}\")\n",
    "        \n",
    "        results[corruption_type] = {\n",
    "            'psnr': avg_psnr,\n",
    "            'ssim': avg_ssim,\n",
    "            'lpips': avg_lpips,\n",
    "            'fid': fid_score,\n",
    "            'inference_time_ms': (inference_time / len(original_images)) * 1000\n",
    "        }\n",
    "        \n",
    "        corruption_bar.set_postfix(psnr=f\"{avg_psnr:.2f}dB\", ssim=f\"{avg_ssim:.4f}\", lpips=f\"{avg_lpips:.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    return results\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Training Function\n",
    "#----------------------------------------------------------------------------------\n",
    "def train_vae(model, train_loader, val_loader, num_epochs=100, lr=1e-4, kld_weight=0.001, \n",
    "              save_dir='/kaggle/working/checkpoints'):\n",
    "    \"\"\"Train the VAE model with multiple corruption types.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs('/kaggle/working/samples', exist_ok=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Track learning rate changes manually\n",
    "    previous_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Lists to store loss values\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Use different corruption types during training\n",
    "    corruption_types = ['mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined']\n",
    "    corruption_functions = {\n",
    "        'mask': apply_mask,\n",
    "        'gaussian': apply_gaussian_noise,\n",
    "        'salt_pepper': apply_salt_pepper_noise,\n",
    "        'blur': apply_gaussian_blur,\n",
    "        'jpeg': apply_jpeg_artifacts,\n",
    "        'combined': apply_combined_corruption\n",
    "    }\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Create overall progress bar for epochs\n",
    "    try:\n",
    "        epoch_bar = tqdm(range(num_epochs), desc=\"Training Progress\", position=0)\n",
    "        \n",
    "        for epoch in epoch_bar:\n",
    "            model.train()\n",
    "            epoch_train_loss = 0\n",
    "            epoch_recon_loss = 0\n",
    "            epoch_kld_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            # Training loop - without detailed per-batch output\n",
    "            for batch in train_loader:\n",
    "                # Move batch to device\n",
    "                if isinstance(batch, list):\n",
    "                    batch = batch[0]\n",
    "                original = batch.to(device)\n",
    "                \n",
    "                # Apply random corruption\n",
    "                corruption_type = random.choice(corruption_types)\n",
    "                corrupted = corruption_functions[corruption_type](original.clone())\n",
    "                \n",
    "                # Forward pass\n",
    "                reconstructed, mu, logvar = model(corrupted)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, recon_loss, kld_loss = vae_loss(reconstructed, original, mu, logvar, kld_weight)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses without printing\n",
    "                epoch_train_loss += loss.item()\n",
    "                epoch_recon_loss += recon_loss.item()\n",
    "                epoch_kld_loss += kld_loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Average training loss for this epoch\n",
    "            avg_train_loss = epoch_train_loss / batch_count\n",
    "            avg_recon_loss = epoch_recon_loss / batch_count\n",
    "            avg_kld_loss = epoch_kld_loss / batch_count\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation - without detailed per-batch output\n",
    "            model.eval()\n",
    "            epoch_val_loss = 0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    if isinstance(batch, list):\n",
    "                        batch = batch[0]\n",
    "                    original = batch.to(device)\n",
    "                    \n",
    "                    # Apply random corruption for validation\n",
    "                    corruption_type = random.choice(corruption_types)\n",
    "                    corrupted = corruption_functions[corruption_type](original.clone())\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    reconstructed, mu, logvar = model(corrupted)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss, _, _ = vae_loss(reconstructed, original, mu, logvar, kld_weight)\n",
    "                    epoch_val_loss += loss.item()\n",
    "                    val_batch_count += 1\n",
    "            \n",
    "            # Average validation loss for this epoch\n",
    "            avg_val_loss = epoch_val_loss / val_batch_count\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            # Update the main epoch progress bar\n",
    "            epoch_bar.set_postfix(train_loss=f\"{avg_train_loss:.4f}\", val_loss=f\"{avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Print epoch summary - ONLY PRINT ONCE PER EPOCH\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "            print(f\" Train Loss: {avg_train_loss:.6f} (Recon: {avg_recon_loss:.6f}, KLD: {avg_kld_loss:.6f})\")\n",
    "            print(f\" Validation Loss: {avg_val_loss:.6f}\")\n",
    "            print(f\" Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'latent_dim': model.latent_dim\n",
    "                }, f\"{save_dir}/vae_epoch_{epoch+1}.pth\")\n",
    "                print(f\"Model checkpoint saved to {save_dir}/vae_epoch_{epoch+1}.pth\")\n",
    "            \n",
    "            # Generate and save sample images\n",
    "            if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "                save_sample_images(model, val_loader, epoch + 1)\n",
    "            \n",
    "            # Evaluate model on validation set (every 5 epochs or at the end)\n",
    "            if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "                print(f\"\\nRunning evaluation at epoch {epoch+1}...\")\n",
    "                eval_results = evaluate_model(model, val_loader)\n",
    "                print(\"\\nValidation Metrics:\")\n",
    "                for corruption_type, metrics in eval_results.items():\n",
    "                    print(f\" {corruption_type}: PSNR = {metrics['psnr']:.2f}dB, SSIM = {metrics['ssim']:.4f}, \" +\n",
    "                        f\"LPIPS = {metrics['lpips']:.4f}, FID = {metrics['fid']:.2f}, \" +\n",
    "                        f\"Inference Time = {metrics['inference_time_ms']:.2f}ms\")\n",
    "                \n",
    "                # Save metrics to file for later analysis\n",
    "                metrics_file = f\"{save_dir}/metrics_epoch_{epoch+1}.json\"\n",
    "                with open(metrics_file, 'w') as f:\n",
    "                    json.dump(eval_results, f, indent=2)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Check for learning rate changes and log them\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if current_lr != previous_lr:\n",
    "                print(f\"\\nLearning rate adjusted from {previous_lr:.6f} to {current_lr:.6f}\")\n",
    "                previous_lr = current_lr\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {e}\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('/kaggle/working/loss_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Save the final model in Kaggle output directory\n",
    "    final_model_path = '/kaggle/working/vae_final_model.pth'\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'latent_dim': model.latent_dim\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    " \n",
    "            \n",
    "         \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Image Restoration and Visualization\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "def restore_image(model, image_path, corruption_type='combined', save_path=None):\n",
    "    \"\"\"Restore a single image using the trained VAE model.\"\"\"\n",
    "    if isinstance(model, dict):\n",
    "        # If model is a state dict, create a new model instance\n",
    "        latent_dim = model.get('latent_dim', 128)\n",
    "        model_instance = VAE(latent_dim=latent_dim).to(device)\n",
    "        model_instance.load_state_dict(model['model_state_dict'])\n",
    "        model = model_instance\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Apply corruption\n",
    "    corruption_functions = {\n",
    "        'mask': apply_mask,\n",
    "        'gaussian': apply_gaussian_noise,\n",
    "        'salt_pepper': apply_salt_pepper_noise,\n",
    "        'blur': apply_gaussian_blur,\n",
    "        'jpeg': apply_jpeg_artifacts,\n",
    "        'combined': apply_combined_corruption\n",
    "    }\n",
    "    \n",
    "    if corruption_type not in corruption_functions:\n",
    "        print(f\"Unknown corruption type: {corruption_type}. Using 'combined' instead.\")\n",
    "        corruption_type = 'combined'\n",
    "    \n",
    "    corrupted = corruption_functions[corruption_type](original.clone())\n",
    "    \n",
    "    # Restore image\n",
    "    with torch.no_grad():\n",
    "        restored, _, _ = model(corrupted)\n",
    "    \n",
    "    # Convert tensors to images for visualization\n",
    "    # Convert from [-1, 1] range to [0, 1]\n",
    "    original_img = original.cpu() * 0.5 + 0.5\n",
    "    corrupted_img = corrupted.cpu() * 0.5 + 0.5\n",
    "    restored_img = restored.cpu() * 0.5 + 0.5\n",
    "    \n",
    "    # Create comparison grid\n",
    "    comparison = torch.cat([original_img, corrupted_img, restored_img], dim=0)\n",
    "    \n",
    "    # Save or display results\n",
    "    if save_path:\n",
    "        save_image(comparison, save_path, nrow=1, padding=5)\n",
    "        print(f\"Restored image saved to {save_path}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    psnr = compute_psnr(original, restored)\n",
    "    ssim = compute_ssim(original, restored)\n",
    "    \n",
    "    # Initialize LPIPS\n",
    "    lpips_metric = LPIPSMetric(device)\n",
    "    lpips_val = lpips_metric.calculate_lpips(original, restored)\n",
    "    \n",
    "    print(f\"Restoration Metrics:\")\n",
    "    print(f\" PSNR: {psnr:.2f}dB\")\n",
    "    print(f\" SSIM: {ssim:.4f}\")\n",
    "    print(f\" LPIPS: {lpips_val:.4f}\")\n",
    "    \n",
    "    # For displaying in notebook\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_img.squeeze().permute(1, 2, 0).numpy())\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(corrupted_img.squeeze().permute(1, 2, 0).numpy())\n",
    "    plt.title(f\"Corrupted ({corruption_type})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(restored_img.squeeze().permute(1, 2, 0).numpy())\n",
    "    plt.title(f\"Restored (PSNR: {psnr:.2f}dB)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return original, corrupted, restored, {'psnr': psnr, 'ssim': ssim, 'lpips': lpips_val}\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Loading Pretrained Model\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "def load_pretrained_model(checkpoint_path):\n",
    "    \"\"\"Load a pretrained VAE model.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Get latent dimension from checkpoint or use default\n",
    "    latent_dim = checkpoint.get('latent_dim', 128)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = VAE(latent_dim=latent_dim).to(device)\n",
    "    \n",
    "    # Load model state\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        # Assume it is a direct state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    print(f\"Loaded pretrained model from {checkpoint_path}\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"Trained for {checkpoint['epoch']} epochs\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Main Pipeline\n",
    "#------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7ea5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:37:57.639390Z",
     "iopub.status.busy": "2025-05-20T17:37:57.639088Z",
     "iopub.status.idle": "2025-05-20T17:37:57.650894Z",
     "shell.execute_reply": "2025-05-20T17:37:57.650298Z"
    },
    "papermill": {
     "duration": 0.016135,
     "end_time": "2025-05-20T17:37:57.651928",
     "exception": false,
     "start_time": "2025-05-20T17:37:57.635793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(max_images=30000, num_epochs=100):\n",
    "    \"\"\"Run the complete pipeline: load data, train model, evaluate and test.\"\"\"\n",
    "    print(\"Starting ClearVision Pipeline...\")\n",
    "    \n",
    "    # Set paths for Kaggle\n",
    "    data_dir = '/kaggle/input/celebahq-resized-256x256/celeba_hq_256'\n",
    "    output_dir = '/kaggle/working'\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(f'{output_dir}/checkpoints', exist_ok=True)\n",
    "    os.makedirs(f'{output_dir}/samples', exist_ok=True)\n",
    "    os.makedirs(f'{output_dir}/metrics', exist_ok=True)\n",
    "    \n",
    "    # Create a progress bar for the main pipeline steps\n",
    "    pipeline_steps = [\"Loading datasets\", \"Model initialization\", \"Training\", \"Evaluation\", \"Saving model\"]\n",
    "    pipeline_bar = tqdm(pipeline_steps, desc=\"ClearVision Pipeline\")\n",
    "    \n",
    "    # Step 1: Load datasets with limit\n",
    "    pipeline_bar.set_description(\"Loading datasets...\")\n",
    "    train_loader, val_loader = load_dataset(data_dir, max_images=max_images)\n",
    "    \n",
    "    if train_loader is None or val_loader is None:\n",
    "        print(\"Failed to load datasets. Please check the dataset paths.\")\n",
    "        return\n",
    "    pipeline_bar.update(1)\n",
    "    \n",
    "    # Step 2: Initialize model\n",
    "    pipeline_bar.set_description(\"Initializing VAE model...\")\n",
    "    latent_dim = 128\n",
    "    model = VAE(latent_dim=latent_dim).to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    pipeline_bar.update(1)\n",
    "    \n",
    "    # Step 3: Train model\n",
    "    pipeline_bar.set_description(\"Training model...\")\n",
    "    train_losses, val_losses = train_vae(model, train_loader, val_loader, \n",
    "                                       num_epochs=num_epochs, \n",
    "                                       lr=1e-4, \n",
    "                                       kld_weight=0.001,\n",
    "                                       save_dir=f'{output_dir}/checkpoints')\n",
    "    pipeline_bar.update(1)\n",
    "    \n",
    "    # Step 4: Final evaluation on validation set\n",
    "    pipeline_bar.set_description(\"Evaluating final model...\")\n",
    "    eval_results = evaluate_model(model, val_loader)\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for corruption_type, metrics in eval_results.items():\n",
    "        print(f\" {corruption_type}: PSNR = {metrics['psnr']:.2f}dB, SSIM = {metrics['ssim']:.4f}, LPIPS = {metrics.get('lpips', 'N/A')}\")\n",
    "        if 'inference_time_ms' in metrics:\n",
    "            print(f\" {corruption_type} Inference Time: {metrics['inference_time_ms']:.2f}ms\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_file = os.path.join(output_dir, 'metrics', 'final_metrics.json')\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        import json\n",
    "        # Convert values that might not be JSON serializable\n",
    "        serializable_results = {}\n",
    "        for k, v in eval_results.items():\n",
    "            serializable_results[k] = {k2: float(v2) if not isinstance(v2, str) else v2 \n",
    "                                     for k2, v2 in v.items()}\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Metrics saved to {metrics_file}\")\n",
    "    pipeline_bar.update(1)\n",
    "    \n",
    "    # Step 5: Save the final model\n",
    "    pipeline_bar.set_description(\"Saving final model...\")\n",
    "    final_model_path = os.path.join(output_dir, 'checkpoints', 'vae_final_model.pth')\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'latent_dim': latent_dim\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    pipeline_bar.update(1)\n",
    "    \n",
    "    pipeline_bar.close()\n",
    "    print(\"\\nClearVision Pipeline completed!\")\n",
    "    \n",
    "    # Create a sample image with all corruption types for the final model\n",
    "    create_final_comparison_image(model, val_loader, \n",
    "                                output_path=os.path.join(output_dir, 'final_results.png'))\n",
    "    \n",
    "    return model, train_loader, val_loader\n",
    "\n",
    "def create_final_comparison_image(model, val_loader, output_path):\n",
    "    \"\"\"Create a final comparison image with all corruption types\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of images\n",
    "    batch = next(iter(val_loader))\n",
    "    if isinstance(batch, list):\n",
    "        batch = batch[0]\n",
    "    original_image = batch[0:1].to(device)  # Use first image\n",
    "    \n",
    "    corruption_types = ['mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined']\n",
    "    corruption_functions = {\n",
    "        'mask': apply_mask,\n",
    "        'gaussian': apply_gaussian_noise,\n",
    "        'salt_pepper': apply_salt_pepper_noise,\n",
    "        'blur': apply_gaussian_blur,\n",
    "        'jpeg': apply_jpeg_artifacts,\n",
    "        'combined': apply_combined_corruption\n",
    "    }\n",
    "    \n",
    "    # Create rows for each corruption type\n",
    "    rows = []\n",
    "    \n",
    "    for corruption_type in corruption_types:\n",
    "        # Apply corruption\n",
    "        corruption_fn = corruption_functions[corruption_type]\n",
    "        corrupted = corruption_fn(original_image.clone())\n",
    "        \n",
    "        # Generate restored image\n",
    "        with torch.no_grad():\n",
    "            restored, _, _ = model(corrupted)\n",
    "        \n",
    "        # Add to rows: original, corrupted, restored\n",
    "        row = torch.cat([original_image, corrupted, restored], dim=0)\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create final grid\n",
    "    grid = torch.cat(rows, dim=0)\n",
    "    save_image(grid * 0.5 + 0.5, output_path, nrow=3)\n",
    "    print(f\"Final comparison image saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43057464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:37:57.657504Z",
     "iopub.status.busy": "2025-05-20T17:37:57.657308Z",
     "iopub.status.idle": "2025-05-20T17:37:57.668045Z",
     "shell.execute_reply": "2025-05-20T17:37:57.667466Z"
    },
    "papermill": {
     "duration": 0.014771,
     "end_time": "2025-05-20T17:37:57.669043",
     "exception": false,
     "start_time": "2025-05-20T17:37:57.654272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model, val_loader, num_samples=5, output_dir='/kaggle/working/test_outputs'):\n",
    "    \"\"\"Test the trained model on the validation set and generate example outputs\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get corruption types\n",
    "    corruption_types = ['mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined']\n",
    "    corruption_functions = {\n",
    "        'mask': apply_mask,\n",
    "        'gaussian': apply_gaussian_noise,\n",
    "        'salt_pepper': apply_salt_pepper_noise,\n",
    "        'blur': apply_gaussian_blur,\n",
    "        'jpeg': apply_jpeg_artifacts,\n",
    "        'combined': apply_combined_corruption\n",
    "    }\n",
    "    \n",
    "    # Evaluate and report metrics for each corruption type\n",
    "    print(\"\\nModel Testing Results:\")\n",
    "    \n",
    "    for corruption_type in corruption_types:\n",
    "        # Get a batch of samples\n",
    "        batch = next(iter(val_loader))\n",
    "        if isinstance(batch, list):\n",
    "            batch = batch[0]\n",
    "        \n",
    "        # Select random samples\n",
    "        indices = torch.randperm(len(batch))[:num_samples]\n",
    "        original_images = batch[indices].to(device)\n",
    "        \n",
    "        # Apply corruption\n",
    "        corruption_fn = corruption_functions[corruption_type]\n",
    "        corrupted_images = corruption_fn(original_images)\n",
    "        \n",
    "        # Generate restored images\n",
    "        with torch.no_grad():\n",
    "            restored_images, _, _ = model(corrupted_images)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_psnr = 0\n",
    "        avg_ssim = 0\n",
    "        lpips_scores = []\n",
    "        \n",
    "        # Initialize LPIPS metric\n",
    "        lpips_metric = LPIPSMetric(device)\n",
    "        \n",
    "        for i in range(len(original_images)):\n",
    "            psnr = compute_psnr(original_images[i:i+1], restored_images[i:i+1])\n",
    "            ssim = compute_ssim(original_images[i:i+1], restored_images[i:i+1])\n",
    "            lpips = lpips_metric.calculate_lpips(original_images[i:i+1], restored_images[i:i+1])\n",
    "            \n",
    "            avg_psnr += psnr\n",
    "            avg_ssim += ssim\n",
    "            lpips_scores.append(lpips)\n",
    "        \n",
    "        avg_psnr /= len(original_images)\n",
    "        avg_ssim /= len(original_images)\n",
    "        avg_lpips = sum(lpips_scores) / len(lpips_scores)\n",
    "        \n",
    "        print(f\"{corruption_type.capitalize()} Corruption:\")\n",
    "        print(f\"  PSNR: {avg_psnr:.2f}dB\")\n",
    "        print(f\"  SSIM: {avg_ssim:.4f}\")\n",
    "        print(f\"  LPIPS: {avg_lpips:.4f}\")\n",
    "        \n",
    "        # Save example images\n",
    "        for i in range(len(original_images)):\n",
    "            comparison = torch.cat([\n",
    "                original_images[i:i+1], \n",
    "                corrupted_images[i:i+1], \n",
    "                restored_images[i:i+1]\n",
    "            ], dim=0)\n",
    "            \n",
    "            save_image(\n",
    "                comparison * 0.5 + 0.5,\n",
    "                f\"{output_dir}/{corruption_type}_sample_{i+1}.png\",\n",
    "                nrow=3, \n",
    "                padding=5\n",
    "            )\n",
    "    \n",
    "    # Create a comprehensive test grid with all corruption types\n",
    "    create_test_grid(model, val_loader, output_dir)\n",
    "    \n",
    "    print(f\"\\nTest images saved to {output_dir}\")\n",
    "    return\n",
    "\n",
    "def create_test_grid(model, val_loader, output_dir):\n",
    "    \"\"\"Create a comprehensive test grid with all corruption types\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of images\n",
    "    batch = next(iter(val_loader))\n",
    "    if isinstance(batch, list):\n",
    "        batch = batch[0]\n",
    "    \n",
    "    # Use 2 sample images\n",
    "    original_images = batch[:2].to(device)\n",
    "    \n",
    "    corruption_types = ['mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined']\n",
    "    corruption_functions = {\n",
    "        'mask': apply_mask,\n",
    "        'gaussian': apply_gaussian_noise,\n",
    "        'salt_pepper': apply_salt_pepper_noise,\n",
    "        'blur': apply_gaussian_blur,\n",
    "        'jpeg': apply_jpeg_artifacts,\n",
    "        'combined': apply_combined_corruption\n",
    "    }\n",
    "    \n",
    "    # Process each image\n",
    "    all_rows = []\n",
    "    for img_idx in range(len(original_images)):\n",
    "        image_rows = []\n",
    "        original_image = original_images[img_idx:img_idx+1]\n",
    "        \n",
    "        for corruption_type in corruption_types:\n",
    "            corruption_fn = corruption_functions[corruption_type]\n",
    "            corrupted = corruption_fn(original_image.clone())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                restored, _, _ = model(corrupted)\n",
    "            \n",
    "            # Create row: original, corrupted, restored\n",
    "            image_rows.append(torch.cat([original_image, corrupted, restored], dim=0))\n",
    "        \n",
    "        # Combine all corruption types for this image\n",
    "        all_rows.append(torch.cat(image_rows, dim=0))\n",
    "    \n",
    "    # Create final grid\n",
    "    grid = torch.cat(all_rows, dim=0)\n",
    "    save_image(grid * 0.5 + 0.5, f\"{output_dir}/comprehensive_test.png\", nrow=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac200a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:37:57.674372Z",
     "iopub.status.busy": "2025-05-20T17:37:57.674190Z",
     "iopub.status.idle": "2025-05-20T19:30:34.946058Z",
     "shell.execute_reply": "2025-05-20T19:30:34.944867Z"
    },
    "papermill": {
     "duration": 6757.276171,
     "end_time": "2025-05-20T19:30:34.947554",
     "exception": false,
     "start_time": "2025-05-20T17:37:57.671383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ClearVision Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing VAE model...:  20%|██        | 1/5 [00:00<00:01,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 30000 images\n",
      "Training set: 27000 images\n",
      "Validation set: 3000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...:  40%|████      | 2/5 [00:00<00:01,  2.88it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 10,828,739\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [01:48<2:59:02, 108.51s/it, train_loss=5607.4079, val_loss=2788.6650]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100:\n",
      " Train Loss: 5607.407859 (Recon: 5428.766137, KLD: 178641.703507)\n",
      " Validation Loss: 2788.664988\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [02:50<2:12:25, 81.07s/it, train_loss=2496.5790, val_loss=2116.8730] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100:\n",
      " Train Loss: 2496.578962 (Recon: 2495.191087, KLD: 1387.874673)\n",
      " Validation Loss: 2116.872968\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [03:51<1:56:24, 72.00s/it, train_loss=2044.8931, val_loss=1846.3450]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100:\n",
      " Train Loss: 2044.893136 (Recon: 2043.044502, KLD: 1848.635398)\n",
      " Validation Loss: 1846.345004\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 4/100 [04:52<1:48:24, 67.76s/it, train_loss=1782.6863, val_loss=1711.0686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100:\n",
      " Train Loss: 1782.686287 (Recon: 1780.432669, KLD: 2253.617979)\n",
      " Validation Loss: 1711.068583\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 5/100 [05:54<1:43:45, 65.53s/it, train_loss=1631.4528, val_loss=1729.6461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100:\n",
      " Train Loss: 1631.452836 (Recon: 1628.768580, KLD: 2684.255406)\n",
      " Validation Loss: 1729.646068\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 6/100 [06:56<1:40:54, 64.41s/it, train_loss=1511.0873, val_loss=1430.4785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100:\n",
      " Train Loss: 1511.087275 (Recon: 1508.032345, KLD: 3054.929541)\n",
      " Validation Loss: 1430.478535\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 7/100 [07:58<1:38:35, 63.60s/it, train_loss=1407.4761, val_loss=1317.8049]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100:\n",
      " Train Loss: 1407.476077 (Recon: 1404.119467, KLD: 3356.609951)\n",
      " Validation Loss: 1317.804862\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 8/100 [09:01<1:36:56, 63.23s/it, train_loss=1357.3072, val_loss=1412.2966]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100:\n",
      " Train Loss: 1357.307206 (Recon: 1353.706891, KLD: 3600.316974)\n",
      " Validation Loss: 1412.296572\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 9/100 [10:03<1:35:28, 62.95s/it, train_loss=1299.5268, val_loss=1296.2718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100:\n",
      " Train Loss: 1299.526834 (Recon: 1295.791007, KLD: 3735.827368)\n",
      " Validation Loss: 1296.271774\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 9/100 [11:05<1:35:28, 62.95s/it, train_loss=1251.0787, val_loss=1307.5158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100:\n",
      " Train Loss: 1251.078733 (Recon: 1247.270206, KLD: 3808.528549)\n",
      " Validation Loss: 1307.515781\n",
      " Learning Rate: 0.000100\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_10.pth\n",
      "Sample images saved for epoch 10\n",
      "\n",
      "Running evaluation at epoch 10...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "\n",
      "  0%|          | 0.00/233M [00:00<?, ?B/s]\u001b[A\n",
      "  7%|▋         | 16.4M/233M [00:00<00:01, 171MB/s]\u001b[A\n",
      " 17%|█▋        | 38.5M/233M [00:00<00:00, 206MB/s]\u001b[A\n",
      " 25%|██▍       | 58.2M/233M [00:00<00:00, 194MB/s]\u001b[A\n",
      " 35%|███▍      | 81.1M/233M [00:00<00:00, 211MB/s]\u001b[A\n",
      " 44%|████▍     | 103M/233M [00:00<00:00, 217MB/s] \u001b[A\n",
      " 54%|█████▎    | 125M/233M [00:00<00:00, 221MB/s]\u001b[A\n",
      " 63%|██████▎   | 148M/233M [00:00<00:00, 227MB/s]\u001b[A\n",
      " 73%|███████▎  | 171M/233M [00:00<00:00, 231MB/s]\u001b[A\n",
      " 83%|████████▎ | 194M/233M [00:00<00:00, 233MB/s]\u001b[A\n",
      "100%|██████████| 233M/233M [00:01<00:00, 224MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "\n",
      "  0%|          | 0.00/104M [00:00<?, ?B/s]\u001b[A\n",
      " 14%|█▍        | 14.8M/104M [00:00<00:00, 154MB/s]\u001b[A\n",
      " 36%|███▌      | 37.6M/104M [00:00<00:00, 205MB/s]\u001b[A\n",
      " 58%|█████▊    | 60.4M/104M [00:00<00:00, 220MB/s]\u001b[A\n",
      "100%|██████████| 104M/104M [00:00<00:00, 222MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.8971, psnr=22.59dB, ssim=0.9486]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:33,  6.79s/it, lpips=1.8971, psnr=22.59dB, ssim=0.9486]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:33,  6.79s/it, lpips=1.8971, psnr=22.59dB, ssim=0.9486]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:13<00:33,  6.79s/it, lpips=1.8075, psnr=23.17dB, ssim=0.9561]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:13<00:26,  6.75s/it, lpips=1.8075, psnr=23.17dB, ssim=0.9561]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:13<00:26,  6.75s/it, lpips=1.8075, psnr=23.17dB, ssim=0.9561]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:19<00:26,  6.75s/it, lpips=1.8587, psnr=23.15dB, ssim=0.9550]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:19<00:19,  6.56s/it, lpips=1.8587, psnr=23.15dB, ssim=0.9550]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:19<00:19,  6.56s/it, lpips=1.8587, psnr=23.15dB, ssim=0.9550]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:26<00:19,  6.56s/it, lpips=2.0039, psnr=23.09dB, ssim=0.9544]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:26<00:13,  6.52s/it, lpips=2.0039, psnr=23.09dB, ssim=0.9544]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:26<00:13,  6.52s/it, lpips=2.0039, psnr=23.09dB, ssim=0.9544]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:33<00:13,  6.52s/it, lpips=1.8164, psnr=23.00dB, ssim=0.9548]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:33<00:06,  6.88s/it, lpips=1.8164, psnr=23.00dB, ssim=0.9548]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:33<00:06,  6.88s/it, lpips=1.8164, psnr=23.00dB, ssim=0.9548]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:40<00:06,  6.88s/it, lpips=1.9229, psnr=22.81dB, ssim=0.9523]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:40<00:00,  6.69s/it, lpips=1.9229, psnr=22.81dB, ssim=0.9523]\n",
      "Training Progress:  10%|█         | 10/100 [11:50<1:54:44, 76.50s/it, train_loss=1251.0787, val_loss=1307.5158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 22.59dB, SSIM = 0.9486, LPIPS = 1.8971, FID = 133.49, Inference Time = 0.38ms\n",
      " gaussian: PSNR = 23.17dB, SSIM = 0.9561, LPIPS = 1.8075, FID = 129.99, Inference Time = 0.38ms\n",
      " salt_pepper: PSNR = 23.15dB, SSIM = 0.9550, LPIPS = 1.8587, FID = 133.01, Inference Time = 0.24ms\n",
      " blur: PSNR = 23.09dB, SSIM = 0.9544, LPIPS = 2.0039, FID = 135.06, Inference Time = 0.23ms\n",
      " jpeg: PSNR = 23.00dB, SSIM = 0.9548, LPIPS = 1.8164, FID = 132.37, Inference Time = 0.35ms\n",
      " combined: PSNR = 22.81dB, SSIM = 0.9523, LPIPS = 1.9229, FID = 137.06, Inference Time = 0.23ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 11/100 [12:52<1:46:58, 72.11s/it, train_loss=1227.3149, val_loss=1198.7621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100:\n",
      " Train Loss: 1227.314933 (Recon: 1223.447247, KLD: 3867.685020)\n",
      " Validation Loss: 1198.762115\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 12/100 [13:55<1:41:49, 69.43s/it, train_loss=1193.6644, val_loss=1233.2580]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100:\n",
      " Train Loss: 1193.664378 (Recon: 1189.811971, KLD: 3852.407494)\n",
      " Validation Loss: 1233.258045\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 13/100 [14:57<1:37:27, 67.21s/it, train_loss=1173.6214, val_loss=1170.4304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100:\n",
      " Train Loss: 1173.621351 (Recon: 1169.808948, KLD: 3812.402380)\n",
      " Validation Loss: 1170.430425\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 14/100 [16:00<1:34:18, 65.80s/it, train_loss=1147.7629, val_loss=1274.8564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100:\n",
      " Train Loss: 1147.762911 (Recon: 1143.933677, KLD: 3829.234574)\n",
      " Validation Loss: 1274.856423\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 15/100 [17:03<1:32:08, 65.05s/it, train_loss=1144.1519, val_loss=1152.2000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100:\n",
      " Train Loss: 1144.151936 (Recon: 1140.367010, KLD: 3784.925272)\n",
      " Validation Loss: 1152.199952\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 16/100 [18:06<1:30:02, 64.32s/it, train_loss=1108.8514, val_loss=1144.9514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/100:\n",
      " Train Loss: 1108.851379 (Recon: 1105.000047, KLD: 3851.331428)\n",
      " Validation Loss: 1144.951417\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 17/100 [19:08<1:28:09, 63.73s/it, train_loss=1093.0114, val_loss=1130.9746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/100:\n",
      " Train Loss: 1093.011368 (Recon: 1089.182703, KLD: 3828.665497)\n",
      " Validation Loss: 1130.974629\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 18/100 [20:11<1:26:37, 63.38s/it, train_loss=1069.7432, val_loss=1130.4002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/100:\n",
      " Train Loss: 1069.743206 (Recon: 1065.940995, KLD: 3802.210974)\n",
      " Validation Loss: 1130.400213\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 19/100 [21:13<1:25:08, 63.07s/it, train_loss=1057.9335, val_loss=1126.9585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/100:\n",
      " Train Loss: 1057.933488 (Recon: 1054.185641, KLD: 3747.847287)\n",
      " Validation Loss: 1126.958545\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 19/100 [22:15<1:25:08, 63.07s/it, train_loss=1043.5253, val_loss=1169.7076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/100:\n",
      " Train Loss: 1043.525285 (Recon: 1039.959054, KLD: 3566.230578)\n",
      " Validation Loss: 1169.707598\n",
      " Learning Rate: 0.000100\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_20.pth\n",
      "Sample images saved for epoch 20\n",
      "\n",
      "Running evaluation at epoch 20...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.7067, psnr=23.47dB, ssim=0.9584]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:31,  6.34s/it, lpips=1.7067, psnr=23.47dB, ssim=0.9584]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:31,  6.34s/it, lpips=1.7067, psnr=23.47dB, ssim=0.9584]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:12<00:31,  6.34s/it, lpips=1.6986, psnr=23.79dB, ssim=0.9612]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:12<00:25,  6.44s/it, lpips=1.6986, psnr=23.79dB, ssim=0.9612]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:12<00:25,  6.44s/it, lpips=1.6986, psnr=23.79dB, ssim=0.9612]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:19<00:25,  6.44s/it, lpips=1.7513, psnr=23.76dB, ssim=0.9601]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:19<00:18,  6.33s/it, lpips=1.7513, psnr=23.76dB, ssim=0.9601]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:19<00:18,  6.33s/it, lpips=1.7513, psnr=23.76dB, ssim=0.9601]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:25<00:18,  6.33s/it, lpips=1.9502, psnr=23.63dB, ssim=0.9588]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:25<00:12,  6.32s/it, lpips=1.9502, psnr=23.63dB, ssim=0.9588]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:25<00:12,  6.32s/it, lpips=1.9502, psnr=23.63dB, ssim=0.9588]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:33<00:12,  6.32s/it, lpips=1.7165, psnr=23.59dB, ssim=0.9599]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:33<00:06,  7.00s/it, lpips=1.7165, psnr=23.59dB, ssim=0.9599]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:33<00:06,  7.00s/it, lpips=1.7165, psnr=23.59dB, ssim=0.9599]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:40<00:06,  7.00s/it, lpips=1.7771, psnr=23.85dB, ssim=0.9593]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:40<00:00,  6.67s/it, lpips=1.7771, psnr=23.85dB, ssim=0.9593]\n",
      "Training Progress:  20%|██        | 20/100 [22:58<1:40:48, 75.61s/it, train_loss=1043.5253, val_loss=1169.7076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 23.47dB, SSIM = 0.9584, LPIPS = 1.7067, FID = 123.28, Inference Time = 0.34ms\n",
      " gaussian: PSNR = 23.79dB, SSIM = 0.9612, LPIPS = 1.6986, FID = 122.65, Inference Time = 0.27ms\n",
      " salt_pepper: PSNR = 23.76dB, SSIM = 0.9601, LPIPS = 1.7513, FID = 123.26, Inference Time = 0.24ms\n",
      " blur: PSNR = 23.63dB, SSIM = 0.9588, LPIPS = 1.9502, FID = 127.43, Inference Time = 0.23ms\n",
      " jpeg: PSNR = 23.59dB, SSIM = 0.9599, LPIPS = 1.7165, FID = 125.63, Inference Time = 0.26ms\n",
      " combined: PSNR = 23.85dB, SSIM = 0.9593, LPIPS = 1.7771, FID = 125.53, Inference Time = 0.24ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|██        | 21/100 [24:00<1:34:18, 71.63s/it, train_loss=1026.1736, val_loss=1112.5511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/100:\n",
      " Train Loss: 1026.173636 (Recon: 1022.563799, KLD: 3609.835974)\n",
      " Validation Loss: 1112.551122\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 22/100 [25:02<1:29:25, 68.79s/it, train_loss=1010.5471, val_loss=1100.6560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/100:\n",
      " Train Loss: 1010.547114 (Recon: 1006.964180, KLD: 3582.934125)\n",
      " Validation Loss: 1100.655990\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██▎       | 23/100 [26:05<1:25:56, 66.97s/it, train_loss=993.0964, val_loss=1116.1715]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/100:\n",
      " Train Loss: 993.096428 (Recon: 989.562336, KLD: 3534.092782)\n",
      " Validation Loss: 1116.171520\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 24/100 [27:08<1:23:15, 65.73s/it, train_loss=977.3512, val_loss=1106.3337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/100:\n",
      " Train Loss: 977.351246 (Recon: 973.867742, KLD: 3483.503845)\n",
      " Validation Loss: 1106.333702\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 25/100 [28:10<1:20:57, 64.77s/it, train_loss=969.1106, val_loss=1129.0477]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/100:\n",
      " Train Loss: 969.110594 (Recon: 965.680288, KLD: 3430.305436)\n",
      " Validation Loss: 1129.047699\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 26/100 [29:13<1:18:59, 64.05s/it, train_loss=959.8862, val_loss=1107.8094]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/100:\n",
      " Train Loss: 959.886204 (Recon: 956.491980, KLD: 3394.223189)\n",
      " Validation Loss: 1107.809439\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 27/100 [30:15<1:17:21, 63.58s/it, train_loss=945.6759, val_loss=1113.3876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/100:\n",
      " Train Loss: 945.675888 (Recon: 942.324690, KLD: 3351.197859)\n",
      " Validation Loss: 1113.387589\n",
      " Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 28/100 [31:18<1:15:51, 63.22s/it, train_loss=937.0472, val_loss=1105.1816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/100:\n",
      " Train Loss: 937.047163 (Recon: 933.732472, KLD: 3314.690824)\n",
      " Validation Loss: 1105.181565\n",
      " Learning Rate: 0.000100\n",
      "\n",
      "Learning rate adjusted from 0.000100 to 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 29/100 [32:20<1:14:34, 63.02s/it, train_loss=884.1299, val_loss=1059.2368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/100:\n",
      " Train Loss: 884.129946 (Recon: 880.967291, KLD: 3162.654265)\n",
      " Validation Loss: 1059.236821\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 29/100 [33:22<1:14:34, 63.02s/it, train_loss=873.8668, val_loss=1067.0756]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/100:\n",
      " Train Loss: 873.866804 (Recon: 870.834612, KLD: 3032.192908)\n",
      " Validation Loss: 1067.075556\n",
      " Learning Rate: 0.000050\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_30.pth\n",
      "Sample images saved for epoch 30\n",
      "\n",
      "Running evaluation at epoch 30...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:07<?, ?it/s, lpips=1.4504, psnr=24.12dB, ssim=0.9628]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:07<00:38,  7.72s/it, lpips=1.4504, psnr=24.12dB, ssim=0.9628]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:07<00:38,  7.72s/it, lpips=1.4504, psnr=24.12dB, ssim=0.9628]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:13<00:38,  7.72s/it, lpips=1.4380, psnr=24.40dB, ssim=0.9647]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:13<00:27,  6.83s/it, lpips=1.4380, psnr=24.40dB, ssim=0.9647]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:13<00:27,  6.83s/it, lpips=1.4380, psnr=24.40dB, ssim=0.9647]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:20<00:27,  6.83s/it, lpips=1.4696, psnr=24.28dB, ssim=0.9636]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:20<00:19,  6.62s/it, lpips=1.4696, psnr=24.28dB, ssim=0.9636]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:20<00:19,  6.62s/it, lpips=1.4696, psnr=24.28dB, ssim=0.9636]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:26<00:19,  6.62s/it, lpips=1.6873, psnr=24.31dB, ssim=0.9630]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:26<00:12,  6.45s/it, lpips=1.6873, psnr=24.31dB, ssim=0.9630]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:26<00:12,  6.45s/it, lpips=1.6873, psnr=24.31dB, ssim=0.9630]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:32<00:12,  6.45s/it, lpips=1.4493, psnr=24.25dB, ssim=0.9637]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:32<00:06,  6.44s/it, lpips=1.4493, psnr=24.25dB, ssim=0.9637]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:32<00:06,  6.44s/it, lpips=1.4493, psnr=24.25dB, ssim=0.9637]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:39<00:06,  6.44s/it, lpips=1.6319, psnr=23.75dB, ssim=0.9582]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:39<00:00,  6.61s/it, lpips=1.6319, psnr=23.75dB, ssim=0.9582]\n",
      "Training Progress:  30%|███       | 30/100 [34:05<1:27:58, 75.40s/it, train_loss=873.8668, val_loss=1067.0756]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 24.12dB, SSIM = 0.9628, LPIPS = 1.4504, FID = 117.69, Inference Time = 0.34ms\n",
      " gaussian: PSNR = 24.40dB, SSIM = 0.9647, LPIPS = 1.4380, FID = 114.94, Inference Time = 0.32ms\n",
      " salt_pepper: PSNR = 24.28dB, SSIM = 0.9636, LPIPS = 1.4696, FID = 116.82, Inference Time = 0.23ms\n",
      " blur: PSNR = 24.31dB, SSIM = 0.9630, LPIPS = 1.6873, FID = 121.86, Inference Time = 0.24ms\n",
      " jpeg: PSNR = 24.25dB, SSIM = 0.9637, LPIPS = 1.4493, FID = 117.03, Inference Time = 0.24ms\n",
      " combined: PSNR = 23.75dB, SSIM = 0.9582, LPIPS = 1.6319, FID = 120.65, Inference Time = 0.34ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███       | 31/100 [35:06<1:21:56, 71.26s/it, train_loss=859.4232, val_loss=1063.2973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/100:\n",
      " Train Loss: 859.423215 (Recon: 856.492334, KLD: 2930.880129)\n",
      " Validation Loss: 1063.297294\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 32/100 [36:08<1:17:32, 68.42s/it, train_loss=856.7144, val_loss=1055.5976]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/100:\n",
      " Train Loss: 856.714441 (Recon: 853.839059, KLD: 2875.382304)\n",
      " Validation Loss: 1055.597626\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 33/100 [37:11<1:14:30, 66.73s/it, train_loss=850.6236, val_loss=1125.1387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/100:\n",
      " Train Loss: 850.623568 (Recon: 847.802097, KLD: 2821.470524)\n",
      " Validation Loss: 1125.138746\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 34/100 [38:13<1:12:01, 65.48s/it, train_loss=846.1860, val_loss=1066.6123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/100:\n",
      " Train Loss: 846.186023 (Recon: 843.404943, KLD: 2781.079229)\n",
      " Validation Loss: 1066.612257\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [39:16<1:10:01, 64.64s/it, train_loss=843.2329, val_loss=1069.4368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/100:\n",
      " Train Loss: 843.232923 (Recon: 840.500012, KLD: 2732.911138)\n",
      " Validation Loss: 1069.436755\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|███▌      | 36/100 [40:18<1:08:07, 63.87s/it, train_loss=832.0369, val_loss=1072.3171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/100:\n",
      " Train Loss: 832.036919 (Recon: 829.333324, KLD: 2703.594794)\n",
      " Validation Loss: 1072.317069\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  37%|███▋      | 37/100 [41:20<1:06:35, 63.42s/it, train_loss=830.2073, val_loss=1084.9783]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/100:\n",
      " Train Loss: 830.207324 (Recon: 827.535574, KLD: 2671.749642)\n",
      " Validation Loss: 1084.978261\n",
      " Learning Rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  38%|███▊      | 38/100 [42:22<1:05:05, 62.99s/it, train_loss=825.8330, val_loss=1082.5119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/100:\n",
      " Train Loss: 825.833006 (Recon: 823.191902, KLD: 2641.102444)\n",
      " Validation Loss: 1082.511888\n",
      " Learning Rate: 0.000050\n",
      "\n",
      "Learning rate adjusted from 0.000050 to 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 39/100 [43:25<1:04:01, 62.97s/it, train_loss=800.0963, val_loss=1074.6966]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/100:\n",
      " Train Loss: 800.096348 (Recon: 797.514680, KLD: 2581.668073)\n",
      " Validation Loss: 1074.696610\n",
      " Learning Rate: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 39/100 [44:27<1:04:01, 62.97s/it, train_loss=793.0694, val_loss=1072.2231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/100:\n",
      " Train Loss: 793.069437 (Recon: 790.541106, KLD: 2528.330693)\n",
      " Validation Loss: 1072.223145\n",
      " Learning Rate: 0.000025\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_40.pth\n",
      "Sample images saved for epoch 40\n",
      "\n",
      "Running evaluation at epoch 40...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.4902, psnr=23.99dB, ssim=0.9606]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:32,  6.53s/it, lpips=1.4902, psnr=23.99dB, ssim=0.9606]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:32,  6.53s/it, lpips=1.4902, psnr=23.99dB, ssim=0.9606]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:13<00:32,  6.53s/it, lpips=1.4221, psnr=24.41dB, ssim=0.9638]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:13<00:27,  6.75s/it, lpips=1.4221, psnr=24.41dB, ssim=0.9638]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:13<00:27,  6.75s/it, lpips=1.4221, psnr=24.41dB, ssim=0.9638]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:19<00:27,  6.75s/it, lpips=1.4790, psnr=24.30dB, ssim=0.9626]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:19<00:19,  6.56s/it, lpips=1.4790, psnr=24.30dB, ssim=0.9626]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:19<00:19,  6.56s/it, lpips=1.4790, psnr=24.30dB, ssim=0.9626]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:26<00:19,  6.56s/it, lpips=1.6770, psnr=24.23dB, ssim=0.9615]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:26<00:13,  6.53s/it, lpips=1.6770, psnr=24.23dB, ssim=0.9615]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:26<00:13,  6.53s/it, lpips=1.6770, psnr=24.23dB, ssim=0.9615]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:32<00:13,  6.53s/it, lpips=1.4500, psnr=24.29dB, ssim=0.9630]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:32<00:06,  6.50s/it, lpips=1.4500, psnr=24.29dB, ssim=0.9630]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:32<00:06,  6.50s/it, lpips=1.4500, psnr=24.29dB, ssim=0.9630]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:39<00:06,  6.50s/it, lpips=1.6270, psnr=24.28dB, ssim=0.9621]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:39<00:00,  6.55s/it, lpips=1.6270, psnr=24.28dB, ssim=0.9621]\n",
      "Training Progress:  40%|████      | 40/100 [45:09<1:15:06, 75.10s/it, train_loss=793.0694, val_loss=1072.2231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 23.99dB, SSIM = 0.9606, LPIPS = 1.4902, FID = 116.27, Inference Time = 0.33ms\n",
      " gaussian: PSNR = 24.41dB, SSIM = 0.9638, LPIPS = 1.4221, FID = 113.60, Inference Time = 0.35ms\n",
      " salt_pepper: PSNR = 24.30dB, SSIM = 0.9626, LPIPS = 1.4790, FID = 114.00, Inference Time = 0.23ms\n",
      " blur: PSNR = 24.23dB, SSIM = 0.9615, LPIPS = 1.6770, FID = 119.22, Inference Time = 0.24ms\n",
      " jpeg: PSNR = 24.29dB, SSIM = 0.9630, LPIPS = 1.4500, FID = 115.70, Inference Time = 0.24ms\n",
      " combined: PSNR = 24.28dB, SSIM = 0.9621, LPIPS = 1.6270, FID = 118.25, Inference Time = 0.24ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  41%|████      | 41/100 [46:11<1:10:05, 71.27s/it, train_loss=790.9943, val_loss=1077.3381]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/100:\n",
      " Train Loss: 790.994319 (Recon: 788.499939, KLD: 2494.381097)\n",
      " Validation Loss: 1077.338091\n",
      " Learning Rate: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  42%|████▏     | 42/100 [47:13<1:06:06, 68.39s/it, train_loss=789.0634, val_loss=1068.2051]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/100:\n",
      " Train Loss: 789.063362 (Recon: 786.599149, KLD: 2464.213096)\n",
      " Validation Loss: 1068.205126\n",
      " Learning Rate: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|████▎     | 43/100 [48:14<1:02:57, 66.27s/it, train_loss=780.8257, val_loss=1071.1597]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/100:\n",
      " Train Loss: 780.825676 (Recon: 778.384271, KLD: 2441.404194)\n",
      " Validation Loss: 1071.159736\n",
      " Learning Rate: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  44%|████▍     | 44/100 [49:16<1:00:41, 65.02s/it, train_loss=778.8317, val_loss=1067.7319]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/100:\n",
      " Train Loss: 778.831724 (Recon: 776.415582, KLD: 2416.141917)\n",
      " Validation Loss: 1067.731906\n",
      " Learning Rate: 0.000025\n",
      "\n",
      "Learning rate adjusted from 0.000025 to 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 45/100 [50:19<58:57, 64.31s/it, train_loss=767.0676, val_loss=1091.3895]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/100:\n",
      " Train Loss: 767.067590 (Recon: 764.678501, KLD: 2389.088922)\n",
      " Validation Loss: 1091.389492\n",
      " Learning Rate: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  46%|████▌     | 46/100 [51:21<57:17, 63.66s/it, train_loss=764.8281, val_loss=1070.1285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/100:\n",
      " Train Loss: 764.828091 (Recon: 762.465959, KLD: 2362.131665)\n",
      " Validation Loss: 1070.128485\n",
      " Learning Rate: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|████▋     | 47/100 [52:23<55:44, 63.10s/it, train_loss=765.0985, val_loss=1071.7205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/100:\n",
      " Train Loss: 765.098530 (Recon: 762.755550, KLD: 2342.979137)\n",
      " Validation Loss: 1071.720500\n",
      " Learning Rate: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  48%|████▊     | 48/100 [53:25<54:22, 62.75s/it, train_loss=760.7555, val_loss=1071.2059]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/100:\n",
      " Train Loss: 760.755509 (Recon: 758.428551, KLD: 2326.957928)\n",
      " Validation Loss: 1071.205893\n",
      " Learning Rate: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|████▉     | 49/100 [54:27<53:06, 62.48s/it, train_loss=758.9905, val_loss=1078.5356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/100:\n",
      " Train Loss: 758.990510 (Recon: 756.678814, KLD: 2311.694652)\n",
      " Validation Loss: 1078.535586\n",
      " Learning Rate: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|████▉     | 49/100 [55:29<53:06, 62.48s/it, train_loss=757.7812, val_loss=1074.1720]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/100:\n",
      " Train Loss: 757.781160 (Recon: 755.483686, KLD: 2297.472796)\n",
      " Validation Loss: 1074.172012\n",
      " Learning Rate: 0.000013\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_50.pth\n",
      "Sample images saved for epoch 50\n",
      "\n",
      "Running evaluation at epoch 50...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.3610, psnr=24.19dB, ssim=0.9627]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:32,  6.43s/it, lpips=1.3610, psnr=24.19dB, ssim=0.9627]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:32,  6.43s/it, lpips=1.3610, psnr=24.19dB, ssim=0.9627]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:12<00:32,  6.43s/it, lpips=1.3622, psnr=24.45dB, ssim=0.9644]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:12<00:25,  6.41s/it, lpips=1.3622, psnr=24.45dB, ssim=0.9644]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:12<00:25,  6.41s/it, lpips=1.3622, psnr=24.45dB, ssim=0.9644]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:21<00:25,  6.41s/it, lpips=1.4139, psnr=24.35dB, ssim=0.9633]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:21<00:22,  7.36s/it, lpips=1.4139, psnr=24.35dB, ssim=0.9633]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:21<00:22,  7.36s/it, lpips=1.4139, psnr=24.35dB, ssim=0.9633]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:27<00:22,  7.36s/it, lpips=1.6099, psnr=24.33dB, ssim=0.9626]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:27<00:13,  6.95s/it, lpips=1.6099, psnr=24.33dB, ssim=0.9626]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:27<00:13,  6.95s/it, lpips=1.6099, psnr=24.33dB, ssim=0.9626]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:34<00:13,  6.95s/it, lpips=1.3872, psnr=24.32dB, ssim=0.9636]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:34<00:06,  6.86s/it, lpips=1.3872, psnr=24.32dB, ssim=0.9636]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:34<00:06,  6.86s/it, lpips=1.3872, psnr=24.32dB, ssim=0.9636]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:40<00:06,  6.86s/it, lpips=1.4915, psnr=24.12dB, ssim=0.9607]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:40<00:00,  6.80s/it, lpips=1.4915, psnr=24.12dB, ssim=0.9607]\n",
      "Training Progress:  50%|█████     | 50/100 [56:12<1:02:54, 75.49s/it, train_loss=757.7812, val_loss=1074.1720]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 24.19dB, SSIM = 0.9627, LPIPS = 1.3610, FID = 113.73, Inference Time = 0.34ms\n",
      " gaussian: PSNR = 24.45dB, SSIM = 0.9644, LPIPS = 1.3622, FID = 115.80, Inference Time = 0.29ms\n",
      " salt_pepper: PSNR = 24.35dB, SSIM = 0.9633, LPIPS = 1.4139, FID = 118.11, Inference Time = 0.23ms\n",
      " blur: PSNR = 24.33dB, SSIM = 0.9626, LPIPS = 1.6099, FID = 121.05, Inference Time = 0.23ms\n",
      " jpeg: PSNR = 24.32dB, SSIM = 0.9636, LPIPS = 1.3872, FID = 117.23, Inference Time = 0.35ms\n",
      " combined: PSNR = 24.12dB, SSIM = 0.9607, LPIPS = 1.4915, FID = 117.04, Inference Time = 0.23ms\n",
      "\n",
      "Learning rate adjusted from 0.000013 to 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|█████     | 51/100 [57:15<58:34, 71.72s/it, train_loss=751.3635, val_loss=1064.4696]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51/100:\n",
      " Train Loss: 751.363517 (Recon: 749.079072, KLD: 2284.445110)\n",
      " Validation Loss: 1064.469630\n",
      " Learning Rate: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 52/100 [58:17<55:00, 68.77s/it, train_loss=752.1818, val_loss=1069.7245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52/100:\n",
      " Train Loss: 752.181800 (Recon: 749.906242, KLD: 2275.558417)\n",
      " Validation Loss: 1069.724479\n",
      " Learning Rate: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  53%|█████▎    | 53/100 [59:19<52:17, 66.76s/it, train_loss=748.4470, val_loss=1075.9531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53/100:\n",
      " Train Loss: 748.446999 (Recon: 746.180868, KLD: 2266.131500)\n",
      " Validation Loss: 1075.953057\n",
      " Learning Rate: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  54%|█████▍    | 54/100 [1:00:21<49:58, 65.19s/it, train_loss=747.1154, val_loss=1071.0401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54/100:\n",
      " Train Loss: 747.115353 (Recon: 744.856952, KLD: 2258.402045)\n",
      " Validation Loss: 1071.040072\n",
      " Learning Rate: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 55/100 [1:01:22<48:00, 64.02s/it, train_loss=748.4092, val_loss=1075.1270]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55/100:\n",
      " Train Loss: 748.409225 (Recon: 746.158842, KLD: 2250.382934)\n",
      " Validation Loss: 1075.127021\n",
      " Learning Rate: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  56%|█████▌    | 56/100 [1:02:23<46:21, 63.21s/it, train_loss=746.6336, val_loss=1078.4251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56/100:\n",
      " Train Loss: 746.633646 (Recon: 744.391016, KLD: 2242.630627)\n",
      " Validation Loss: 1078.425059\n",
      " Learning Rate: 0.000006\n",
      "\n",
      "Learning rate adjusted from 0.000006 to 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  57%|█████▋    | 57/100 [1:03:25<45:01, 62.82s/it, train_loss=741.8388, val_loss=1065.3149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57/100:\n",
      " Train Loss: 741.838825 (Recon: 739.601034, KLD: 2237.790630)\n",
      " Validation Loss: 1065.314874\n",
      " Learning Rate: 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  58%|█████▊    | 58/100 [1:04:27<43:45, 62.52s/it, train_loss=745.8267, val_loss=1064.4793]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58/100:\n",
      " Train Loss: 745.826710 (Recon: 743.592040, KLD: 2234.670260)\n",
      " Validation Loss: 1064.479258\n",
      " Learning Rate: 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  59%|█████▉    | 59/100 [1:05:29<42:38, 62.40s/it, train_loss=741.4572, val_loss=1065.6586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59/100:\n",
      " Train Loss: 741.457167 (Recon: 739.226481, KLD: 2230.686591)\n",
      " Validation Loss: 1065.658561\n",
      " Learning Rate: 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  59%|█████▉    | 59/100 [1:06:31<42:38, 62.40s/it, train_loss=740.6482, val_loss=1071.8182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60/100:\n",
      " Train Loss: 740.648163 (Recon: 738.422474, KLD: 2225.688625)\n",
      " Validation Loss: 1071.818239\n",
      " Learning Rate: 0.000003\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_60.pth\n",
      "Sample images saved for epoch 60\n",
      "\n",
      "Running evaluation at epoch 60...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.3668, psnr=24.05dB, ssim=0.9617]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:32,  6.42s/it, lpips=1.3668, psnr=24.05dB, ssim=0.9617]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:32,  6.42s/it, lpips=1.3668, psnr=24.05dB, ssim=0.9617]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:13<00:32,  6.42s/it, lpips=1.3285, psnr=24.44dB, ssim=0.9646]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:13<00:26,  6.52s/it, lpips=1.3285, psnr=24.44dB, ssim=0.9646]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:13<00:26,  6.52s/it, lpips=1.3285, psnr=24.44dB, ssim=0.9646]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:19<00:26,  6.52s/it, lpips=1.3776, psnr=24.40dB, ssim=0.9639]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:19<00:19,  6.43s/it, lpips=1.3776, psnr=24.40dB, ssim=0.9639]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:19<00:19,  6.43s/it, lpips=1.3776, psnr=24.40dB, ssim=0.9639]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:26<00:19,  6.43s/it, lpips=1.5559, psnr=24.38dB, ssim=0.9633]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:26<00:13,  6.76s/it, lpips=1.5559, psnr=24.38dB, ssim=0.9633]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:26<00:13,  6.76s/it, lpips=1.5559, psnr=24.38dB, ssim=0.9633]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:33<00:13,  6.76s/it, lpips=1.3458, psnr=24.32dB, ssim=0.9639]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:33<00:06,  6.67s/it, lpips=1.3458, psnr=24.32dB, ssim=0.9639]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:33<00:06,  6.67s/it, lpips=1.3458, psnr=24.32dB, ssim=0.9639]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:39<00:06,  6.67s/it, lpips=1.3553, psnr=24.07dB, ssim=0.9620]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:39<00:00,  6.57s/it, lpips=1.3553, psnr=24.07dB, ssim=0.9620]\n",
      "Training Progress:  60%|██████    | 60/100 [1:07:13<49:55, 74.88s/it, train_loss=740.6482, val_loss=1071.8182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 24.05dB, SSIM = 0.9617, LPIPS = 1.3668, FID = 114.49, Inference Time = 0.38ms\n",
      " gaussian: PSNR = 24.44dB, SSIM = 0.9646, LPIPS = 1.3285, FID = 114.46, Inference Time = 0.32ms\n",
      " salt_pepper: PSNR = 24.40dB, SSIM = 0.9639, LPIPS = 1.3776, FID = 116.44, Inference Time = 0.24ms\n",
      " blur: PSNR = 24.38dB, SSIM = 0.9633, LPIPS = 1.5559, FID = 119.59, Inference Time = 0.25ms\n",
      " jpeg: PSNR = 24.32dB, SSIM = 0.9639, LPIPS = 1.3458, FID = 116.57, Inference Time = 0.24ms\n",
      " combined: PSNR = 24.07dB, SSIM = 0.9620, LPIPS = 1.3553, FID = 114.99, Inference Time = 0.29ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  61%|██████    | 61/100 [1:08:15<46:05, 70.91s/it, train_loss=740.6496, val_loss=1078.1531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61/100:\n",
      " Train Loss: 740.649626 (Recon: 738.428188, KLD: 2221.438305)\n",
      " Validation Loss: 1078.153092\n",
      " Learning Rate: 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  62%|██████▏   | 62/100 [1:09:18<43:22, 68.48s/it, train_loss=743.1301, val_loss=1080.1300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62/100:\n",
      " Train Loss: 743.130091 (Recon: 740.913848, KLD: 2216.243213)\n",
      " Validation Loss: 1080.129982\n",
      " Learning Rate: 0.000003\n",
      "\n",
      "Learning rate adjusted from 0.000003 to 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  63%|██████▎   | 63/100 [1:10:21<41:12, 66.83s/it, train_loss=736.5861, val_loss=1070.5980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63/100:\n",
      " Train Loss: 736.586080 (Recon: 734.372604, KLD: 2213.476462)\n",
      " Validation Loss: 1070.597961\n",
      " Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|██████▍   | 64/100 [1:11:23<39:19, 65.54s/it, train_loss=739.7717, val_loss=1079.7443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64/100:\n",
      " Train Loss: 739.771659 (Recon: 737.560000, KLD: 2211.659763)\n",
      " Validation Loss: 1079.744338\n",
      " Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 65/100 [1:12:26<37:48, 64.82s/it, train_loss=738.1664, val_loss=1076.1722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65/100:\n",
      " Train Loss: 738.166364 (Recon: 735.956535, KLD: 2209.829346)\n",
      " Validation Loss: 1076.172159\n",
      " Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  66%|██████▌   | 66/100 [1:13:30<36:28, 64.36s/it, train_loss=740.1339, val_loss=1072.5933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66/100:\n",
      " Train Loss: 740.133915 (Recon: 737.926844, KLD: 2207.071979)\n",
      " Validation Loss: 1072.593333\n",
      " Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 67/100 [1:14:32<35:02, 63.71s/it, train_loss=738.4122, val_loss=1067.7059]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67/100:\n",
      " Train Loss: 738.412161 (Recon: 736.205497, KLD: 2206.663377)\n",
      " Validation Loss: 1067.705879\n",
      " Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  68%|██████▊   | 68/100 [1:15:35<33:50, 63.44s/it, train_loss=740.9649, val_loss=1074.2909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68/100:\n",
      " Train Loss: 740.964942 (Recon: 738.761730, KLD: 2203.212947)\n",
      " Validation Loss: 1074.290882\n",
      " Learning Rate: 0.000002\n",
      "\n",
      "Learning rate adjusted from 0.000002 to 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  69%|██████▉   | 69/100 [1:16:38<32:42, 63.31s/it, train_loss=734.9225, val_loss=1073.7082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69/100:\n",
      " Train Loss: 734.922488 (Recon: 732.719282, KLD: 2203.205683)\n",
      " Validation Loss: 1073.708150\n",
      " Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  69%|██████▉   | 69/100 [1:17:40<32:42, 63.31s/it, train_loss=737.2982, val_loss=1074.3075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70/100:\n",
      " Train Loss: 737.298221 (Recon: 735.096414, KLD: 2201.808016)\n",
      " Validation Loss: 1074.307477\n",
      " Learning Rate: 0.000001\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_70.pth\n",
      "Sample images saved for epoch 70\n",
      "\n",
      "Running evaluation at epoch 70...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.3884, psnr=24.13dB, ssim=0.9620]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:31,  6.30s/it, lpips=1.3884, psnr=24.13dB, ssim=0.9620]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:31,  6.30s/it, lpips=1.3884, psnr=24.13dB, ssim=0.9620]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:12<00:31,  6.30s/it, lpips=1.3612, psnr=24.47dB, ssim=0.9649]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:12<00:25,  6.48s/it, lpips=1.3612, psnr=24.47dB, ssim=0.9649]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:12<00:25,  6.48s/it, lpips=1.3612, psnr=24.47dB, ssim=0.9649]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:19<00:25,  6.48s/it, lpips=1.3964, psnr=24.43dB, ssim=0.9640]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:19<00:19,  6.45s/it, lpips=1.3964, psnr=24.43dB, ssim=0.9640]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:19<00:19,  6.45s/it, lpips=1.3964, psnr=24.43dB, ssim=0.9640]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:26<00:19,  6.45s/it, lpips=1.6057, psnr=24.34dB, ssim=0.9631]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:26<00:13,  6.56s/it, lpips=1.6057, psnr=24.34dB, ssim=0.9631]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:26<00:13,  6.56s/it, lpips=1.6057, psnr=24.34dB, ssim=0.9631]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:32<00:13,  6.56s/it, lpips=1.3833, psnr=24.32dB, ssim=0.9640]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:32<00:06,  6.58s/it, lpips=1.3833, psnr=24.32dB, ssim=0.9640]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:32<00:06,  6.58s/it, lpips=1.3833, psnr=24.32dB, ssim=0.9640]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:39<00:06,  6.58s/it, lpips=1.6236, psnr=23.78dB, ssim=0.9582]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:39<00:00,  6.52s/it, lpips=1.6236, psnr=23.78dB, ssim=0.9582]\n",
      "Training Progress:  70%|███████   | 70/100 [1:18:22<37:49, 75.66s/it, train_loss=737.2982, val_loss=1074.3075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 24.13dB, SSIM = 0.9620, LPIPS = 1.3884, FID = 114.91, Inference Time = 0.36ms\n",
      " gaussian: PSNR = 24.47dB, SSIM = 0.9649, LPIPS = 1.3612, FID = 116.44, Inference Time = 0.27ms\n",
      " salt_pepper: PSNR = 24.43dB, SSIM = 0.9640, LPIPS = 1.3964, FID = 118.15, Inference Time = 0.24ms\n",
      " blur: PSNR = 24.34dB, SSIM = 0.9631, LPIPS = 1.6057, FID = 120.44, Inference Time = 0.25ms\n",
      " jpeg: PSNR = 24.32dB, SSIM = 0.9640, LPIPS = 1.3833, FID = 117.66, Inference Time = 0.24ms\n",
      " combined: PSNR = 23.78dB, SSIM = 0.9582, LPIPS = 1.6236, FID = 119.72, Inference Time = 0.30ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|███████   | 71/100 [1:19:25<34:39, 71.72s/it, train_loss=734.2165, val_loss=1078.0374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71/100:\n",
      " Train Loss: 734.216494 (Recon: 732.015749, KLD: 2200.745309)\n",
      " Validation Loss: 1078.037360\n",
      " Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  72%|███████▏  | 72/100 [1:20:26<32:04, 68.75s/it, train_loss=736.7036, val_loss=1081.3611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72/100:\n",
      " Train Loss: 736.703596 (Recon: 734.505046, KLD: 2198.549995)\n",
      " Validation Loss: 1081.361102\n",
      " Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  73%|███████▎  | 73/100 [1:21:28<29:56, 66.56s/it, train_loss=734.9414, val_loss=1081.1134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73/100:\n",
      " Train Loss: 734.941378 (Recon: 732.743737, KLD: 2197.640597)\n",
      " Validation Loss: 1081.113400\n",
      " Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  74%|███████▍  | 74/100 [1:22:29<28:07, 64.89s/it, train_loss=734.4293, val_loss=1069.9745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74/100:\n",
      " Train Loss: 734.429314 (Recon: 732.232666, KLD: 2196.648457)\n",
      " Validation Loss: 1069.974507\n",
      " Learning Rate: 0.000001\n",
      "\n",
      "Learning rate adjusted from 0.000001 to 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 75/100 [1:23:30<26:32, 63.69s/it, train_loss=732.9946, val_loss=1083.4639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75/100:\n",
      " Train Loss: 732.994600 (Recon: 730.798937, KLD: 2195.663213)\n",
      " Validation Loss: 1083.463950\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  76%|███████▌  | 76/100 [1:24:32<25:16, 63.18s/it, train_loss=734.9359, val_loss=1075.1659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76/100:\n",
      " Train Loss: 734.935852 (Recon: 732.740506, KLD: 2195.346172)\n",
      " Validation Loss: 1075.165867\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  77%|███████▋  | 77/100 [1:25:33<24:02, 62.71s/it, train_loss=736.8772, val_loss=1077.4536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77/100:\n",
      " Train Loss: 736.877168 (Recon: 734.683155, KLD: 2194.012734)\n",
      " Validation Loss: 1077.453588\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  78%|███████▊  | 78/100 [1:26:35<22:54, 62.48s/it, train_loss=735.4997, val_loss=1080.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78/100:\n",
      " Train Loss: 735.499717 (Recon: 733.305229, KLD: 2194.488152)\n",
      " Validation Loss: 1080.077371\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  79%|███████▉  | 79/100 [1:27:38<21:54, 62.58s/it, train_loss=735.9558, val_loss=1075.8058]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79/100:\n",
      " Train Loss: 735.955815 (Recon: 733.762493, KLD: 2193.322083)\n",
      " Validation Loss: 1075.805760\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  79%|███████▉  | 79/100 [1:28:40<21:54, 62.58s/it, train_loss=735.6809, val_loss=1080.8910]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80/100:\n",
      " Train Loss: 735.680904 (Recon: 733.487659, KLD: 2193.244013)\n",
      " Validation Loss: 1080.891018\n",
      " Learning Rate: 0.000000\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_80.pth\n",
      "Sample images saved for epoch 80\n",
      "\n",
      "Running evaluation at epoch 80...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:08<?, ?it/s, lpips=1.3472, psnr=23.87dB, ssim=0.9608]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:08<00:42,  8.40s/it, lpips=1.3472, psnr=23.87dB, ssim=0.9608]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:08<00:42,  8.40s/it, lpips=1.3472, psnr=23.87dB, ssim=0.9608]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:14<00:42,  8.40s/it, lpips=1.3232, psnr=24.45dB, ssim=0.9647]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:14<00:28,  7.13s/it, lpips=1.3232, psnr=24.45dB, ssim=0.9647]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:14<00:28,  7.13s/it, lpips=1.3232, psnr=24.45dB, ssim=0.9647]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:20<00:28,  7.13s/it, lpips=1.3473, psnr=24.40dB, ssim=0.9640]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:20<00:20,  6.76s/it, lpips=1.3473, psnr=24.40dB, ssim=0.9640]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:20<00:20,  6.76s/it, lpips=1.3473, psnr=24.40dB, ssim=0.9640]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:27<00:20,  6.76s/it, lpips=1.5478, psnr=24.37dB, ssim=0.9634]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:27<00:13,  6.57s/it, lpips=1.5478, psnr=24.37dB, ssim=0.9634]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:27<00:13,  6.57s/it, lpips=1.5478, psnr=24.37dB, ssim=0.9634]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:33<00:13,  6.57s/it, lpips=1.3412, psnr=24.29dB, ssim=0.9639]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:33<00:06,  6.51s/it, lpips=1.3412, psnr=24.29dB, ssim=0.9639]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:33<00:06,  6.51s/it, lpips=1.3412, psnr=24.29dB, ssim=0.9639]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:41<00:06,  6.51s/it, lpips=1.4941, psnr=24.18dB, ssim=0.9612]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:41<00:00,  6.91s/it, lpips=1.4941, psnr=24.18dB, ssim=0.9612]\n",
      "Training Progress:  80%|████████  | 80/100 [1:29:25<25:14, 75.72s/it, train_loss=735.6809, val_loss=1080.8910]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 23.87dB, SSIM = 0.9608, LPIPS = 1.3472, FID = 111.78, Inference Time = 0.34ms\n",
      " gaussian: PSNR = 24.45dB, SSIM = 0.9647, LPIPS = 1.3232, FID = 114.05, Inference Time = 0.30ms\n",
      " salt_pepper: PSNR = 24.40dB, SSIM = 0.9640, LPIPS = 1.3473, FID = 115.66, Inference Time = 0.23ms\n",
      " blur: PSNR = 24.37dB, SSIM = 0.9634, LPIPS = 1.5478, FID = 119.19, Inference Time = 0.23ms\n",
      " jpeg: PSNR = 24.29dB, SSIM = 0.9639, LPIPS = 1.3412, FID = 116.26, Inference Time = 0.24ms\n",
      " combined: PSNR = 24.18dB, SSIM = 0.9612, LPIPS = 1.4941, FID = 118.38, Inference Time = 0.24ms\n",
      "\n",
      "Learning rate adjusted from 0.000000 to 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  81%|████████  | 81/100 [1:30:27<22:42, 71.69s/it, train_loss=733.3484, val_loss=1083.1689]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81/100:\n",
      " Train Loss: 733.348382 (Recon: 731.155779, KLD: 2192.603260)\n",
      " Validation Loss: 1083.168926\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  82%|████████▏ | 82/100 [1:31:30<20:43, 69.10s/it, train_loss=736.4258, val_loss=1077.1343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82/100:\n",
      " Train Loss: 736.425763 (Recon: 734.233804, KLD: 2191.958167)\n",
      " Validation Loss: 1077.134342\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|████████▎ | 83/100 [1:32:32<19:01, 67.15s/it, train_loss=734.7262, val_loss=1071.2673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83/100:\n",
      " Train Loss: 734.726210 (Recon: 732.534050, KLD: 2192.159798)\n",
      " Validation Loss: 1071.267336\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▍ | 84/100 [1:33:35<17:33, 65.87s/it, train_loss=733.8777, val_loss=1077.6119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84/100:\n",
      " Train Loss: 733.877671 (Recon: 731.685073, KLD: 2192.599432)\n",
      " Validation Loss: 1077.611885\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 85/100 [1:34:39<16:18, 65.24s/it, train_loss=736.2459, val_loss=1081.5715]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85/100:\n",
      " Train Loss: 736.245925 (Recon: 734.054865, KLD: 2191.059469)\n",
      " Validation Loss: 1081.571520\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  86%|████████▌ | 86/100 [1:35:42<15:03, 64.51s/it, train_loss=736.0862, val_loss=1072.7279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86/100:\n",
      " Train Loss: 736.086219 (Recon: 733.895326, KLD: 2190.893765)\n",
      " Validation Loss: 1072.727900\n",
      " Learning Rate: 0.000000\n",
      "\n",
      "Learning rate adjusted from 0.000000 to 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  87%|████████▋ | 87/100 [1:36:45<13:53, 64.09s/it, train_loss=735.6711, val_loss=1080.0843]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87/100:\n",
      " Train Loss: 735.671112 (Recon: 733.480292, KLD: 2190.819696)\n",
      " Validation Loss: 1080.084329\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  88%|████████▊ | 88/100 [1:37:48<12:44, 63.70s/it, train_loss=733.3625, val_loss=1071.6924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88/100:\n",
      " Train Loss: 733.362480 (Recon: 731.171363, KLD: 2191.117211)\n",
      " Validation Loss: 1071.692382\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  89%|████████▉ | 89/100 [1:38:51<11:37, 63.42s/it, train_loss=733.8557, val_loss=1087.1347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89/100:\n",
      " Train Loss: 733.855741 (Recon: 731.664512, KLD: 2191.229531)\n",
      " Validation Loss: 1087.134687\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  89%|████████▉ | 89/100 [1:39:52<11:37, 63.42s/it, train_loss=735.8126, val_loss=1077.5064]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90/100:\n",
      " Train Loss: 735.812601 (Recon: 733.623389, KLD: 2189.211017)\n",
      " Validation Loss: 1077.506389\n",
      " Learning Rate: 0.000000\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_90.pth\n",
      "Sample images saved for epoch 90\n",
      "\n",
      "Running evaluation at epoch 90...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:08<?, ?it/s, lpips=1.4060, psnr=23.96dB, ssim=0.9615]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:08<00:41,  8.38s/it, lpips=1.4060, psnr=23.96dB, ssim=0.9615]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:08<00:41,  8.38s/it, lpips=1.4060, psnr=23.96dB, ssim=0.9615]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:14<00:41,  8.38s/it, lpips=1.3477, psnr=24.47dB, ssim=0.9648]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:14<00:28,  7.07s/it, lpips=1.3477, psnr=24.47dB, ssim=0.9648]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:14<00:28,  7.07s/it, lpips=1.3477, psnr=24.47dB, ssim=0.9648]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:20<00:28,  7.07s/it, lpips=1.3786, psnr=24.40dB, ssim=0.9637]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:20<00:20,  6.68s/it, lpips=1.3786, psnr=24.40dB, ssim=0.9637]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:20<00:20,  6.68s/it, lpips=1.3786, psnr=24.40dB, ssim=0.9637]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:27<00:20,  6.68s/it, lpips=1.5830, psnr=24.33dB, ssim=0.9629]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:27<00:13,  6.52s/it, lpips=1.5830, psnr=24.33dB, ssim=0.9629]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:27<00:13,  6.52s/it, lpips=1.5830, psnr=24.33dB, ssim=0.9629]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:33<00:13,  6.52s/it, lpips=1.3650, psnr=24.33dB, ssim=0.9638]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:33<00:06,  6.51s/it, lpips=1.3650, psnr=24.33dB, ssim=0.9638]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:33<00:06,  6.51s/it, lpips=1.3650, psnr=24.33dB, ssim=0.9638]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:41<00:06,  6.51s/it, lpips=1.5622, psnr=24.23dB, ssim=0.9623]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:41<00:00,  6.83s/it, lpips=1.5622, psnr=24.23dB, ssim=0.9623]\n",
      "Training Progress:  90%|█████████ | 90/100 [1:40:35<12:38, 75.80s/it, train_loss=735.8126, val_loss=1077.5064]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 23.96dB, SSIM = 0.9615, LPIPS = 1.4060, FID = 114.35, Inference Time = 0.35ms\n",
      " gaussian: PSNR = 24.47dB, SSIM = 0.9648, LPIPS = 1.3477, FID = 115.13, Inference Time = 0.33ms\n",
      " salt_pepper: PSNR = 24.40dB, SSIM = 0.9637, LPIPS = 1.3786, FID = 115.85, Inference Time = 0.25ms\n",
      " blur: PSNR = 24.33dB, SSIM = 0.9629, LPIPS = 1.5830, FID = 120.29, Inference Time = 0.23ms\n",
      " jpeg: PSNR = 24.33dB, SSIM = 0.9638, LPIPS = 1.3650, FID = 117.27, Inference Time = 0.23ms\n",
      " combined: PSNR = 24.23dB, SSIM = 0.9623, LPIPS = 1.5622, FID = 119.57, Inference Time = 0.36ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████ | 91/100 [1:41:37<10:43, 71.47s/it, train_loss=733.5858, val_loss=1077.4575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91/100:\n",
      " Train Loss: 733.585789 (Recon: 731.395100, KLD: 2190.688289)\n",
      " Validation Loss: 1077.457543\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  92%|█████████▏| 92/100 [1:42:39<09:09, 68.75s/it, train_loss=735.1114, val_loss=1079.3673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92/100:\n",
      " Train Loss: 735.111386 (Recon: 732.921307, KLD: 2190.079028)\n",
      " Validation Loss: 1079.367345\n",
      " Learning Rate: 0.000000\n",
      "\n",
      "Learning rate adjusted from 0.000000 to 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  93%|█████████▎| 93/100 [1:43:40<07:45, 66.51s/it, train_loss=735.9137, val_loss=1083.0168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93/100:\n",
      " Train Loss: 735.913732 (Recon: 733.724095, KLD: 2189.635968)\n",
      " Validation Loss: 1083.016845\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  94%|█████████▍| 94/100 [1:44:42<06:31, 65.18s/it, train_loss=730.7899, val_loss=1075.2229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94/100:\n",
      " Train Loss: 730.789902 (Recon: 728.598984, KLD: 2190.917361)\n",
      " Validation Loss: 1075.222917\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 95/100 [1:45:44<05:21, 64.21s/it, train_loss=732.7542, val_loss=1078.1843]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95/100:\n",
      " Train Loss: 732.754175 (Recon: 730.563917, KLD: 2190.256867)\n",
      " Validation Loss: 1078.184287\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  96%|█████████▌| 96/100 [1:46:48<04:15, 63.89s/it, train_loss=732.6911, val_loss=1074.7921]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96/100:\n",
      " Train Loss: 732.691053 (Recon: 730.501127, KLD: 2189.925704)\n",
      " Validation Loss: 1074.792131\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  97%|█████████▋| 97/100 [1:47:51<03:11, 63.81s/it, train_loss=731.7401, val_loss=1083.5732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97/100:\n",
      " Train Loss: 731.740122 (Recon: 729.549243, KLD: 2190.879344)\n",
      " Validation Loss: 1083.573236\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  98%|█████████▊| 98/100 [1:48:55<02:07, 63.77s/it, train_loss=734.1329, val_loss=1085.2506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98/100:\n",
      " Train Loss: 734.132929 (Recon: 731.943395, KLD: 2189.533941)\n",
      " Validation Loss: 1085.250553\n",
      " Learning Rate: 0.000000\n",
      "\n",
      "Learning rate adjusted from 0.000000 to 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  99%|█████████▉| 99/100 [1:49:58<01:03, 63.69s/it, train_loss=735.8425, val_loss=1082.1703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99/100:\n",
      " Train Loss: 735.842501 (Recon: 733.653220, KLD: 2189.281521)\n",
      " Validation Loss: 1082.170311\n",
      " Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  99%|█████████▉| 99/100 [1:51:02<01:03, 63.69s/it, train_loss=733.7148, val_loss=1079.8387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100/100:\n",
      " Train Loss: 733.714842 (Recon: 731.524655, KLD: 2190.187880)\n",
      " Validation Loss: 1079.838712\n",
      " Learning Rate: 0.000000\n",
      "Model checkpoint saved to /kaggle/working/checkpoints/vae_epoch_100.pth\n",
      "Sample images saved for epoch 100\n",
      "\n",
      "Running evaluation at epoch 100...\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:08<?, ?it/s, lpips=1.3552, psnr=24.17dB, ssim=0.9627]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:08<00:43,  8.67s/it, lpips=1.3552, psnr=24.17dB, ssim=0.9627]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:08<00:43,  8.67s/it, lpips=1.3552, psnr=24.17dB, ssim=0.9627]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:15<00:43,  8.67s/it, lpips=1.3444, psnr=24.45dB, ssim=0.9648]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:15<00:29,  7.40s/it, lpips=1.3444, psnr=24.45dB, ssim=0.9648]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:15<00:29,  7.40s/it, lpips=1.3444, psnr=24.45dB, ssim=0.9648]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:21<00:29,  7.40s/it, lpips=1.3789, psnr=24.39dB, ssim=0.9638]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:21<00:20,  6.98s/it, lpips=1.3789, psnr=24.39dB, ssim=0.9638]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:21<00:20,  6.98s/it, lpips=1.3789, psnr=24.39dB, ssim=0.9638]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:28<00:20,  6.98s/it, lpips=1.5770, psnr=24.33dB, ssim=0.9632]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:28<00:13,  6.78s/it, lpips=1.5770, psnr=24.33dB, ssim=0.9632]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:28<00:13,  6.78s/it, lpips=1.5770, psnr=24.33dB, ssim=0.9632]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:34<00:13,  6.78s/it, lpips=1.3605, psnr=24.30dB, ssim=0.9639]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:34<00:06,  6.80s/it, lpips=1.3605, psnr=24.30dB, ssim=0.9639]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:34<00:06,  6.80s/it, lpips=1.3605, psnr=24.30dB, ssim=0.9639]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:41<00:06,  6.80s/it, lpips=1.3700, psnr=24.07dB, ssim=0.9622]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:41<00:00,  6.95s/it, lpips=1.3700, psnr=24.07dB, ssim=0.9622]\n",
      "Training Progress: 100%|██████████| 100/100 [1:51:47<00:00, 67.07s/it, train_loss=733.7148, val_loss=1079.8387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      " mask: PSNR = 24.17dB, SSIM = 0.9627, LPIPS = 1.3552, FID = 113.22, Inference Time = 0.34ms\n",
      " gaussian: PSNR = 24.45dB, SSIM = 0.9648, LPIPS = 1.3444, FID = 114.78, Inference Time = 0.26ms\n",
      " salt_pepper: PSNR = 24.39dB, SSIM = 0.9638, LPIPS = 1.3789, FID = 115.82, Inference Time = 0.24ms\n",
      " blur: PSNR = 24.33dB, SSIM = 0.9632, LPIPS = 1.5770, FID = 119.22, Inference Time = 0.24ms\n",
      " jpeg: PSNR = 24.30dB, SSIM = 0.9639, LPIPS = 1.3605, FID = 116.62, Inference Time = 0.26ms\n",
      " combined: PSNR = 24.07dB, SSIM = 0.9622, LPIPS = 1.3700, FID = 115.89, Inference Time = 0.29ms\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating final model...:  60%|██████    | 3/5 [1:51:48<1:42:05, 3062.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to /kaggle/working/vae_final_model.pth\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Inception model loaded for FID calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption types:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:00<?, ?it/s]            \u001b[A\n",
      "Evaluating mask:   0%|          | 0/6 [00:06<?, ?it/s, lpips=1.4007, psnr=24.07dB, ssim=0.9617]\u001b[A\n",
      "Evaluating mask:  17%|█▋        | 1/6 [00:06<00:33,  6.69s/it, lpips=1.4007, psnr=24.07dB, ssim=0.9617]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:06<00:33,  6.69s/it, lpips=1.4007, psnr=24.07dB, ssim=0.9617]\u001b[A\n",
      "Evaluating gaussian:  17%|█▋        | 1/6 [00:13<00:33,  6.69s/it, lpips=1.3409, psnr=24.45dB, ssim=0.9649]\u001b[A\n",
      "Evaluating gaussian:  33%|███▎      | 2/6 [00:13<00:26,  6.65s/it, lpips=1.3409, psnr=24.45dB, ssim=0.9649]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:13<00:26,  6.65s/it, lpips=1.3409, psnr=24.45dB, ssim=0.9649]\u001b[A\n",
      "Evaluating salt_pepper:  33%|███▎      | 2/6 [00:19<00:26,  6.65s/it, lpips=1.3851, psnr=24.40dB, ssim=0.9639]\u001b[A\n",
      "Evaluating salt_pepper:  50%|█████     | 3/6 [00:19<00:19,  6.62s/it, lpips=1.3851, psnr=24.40dB, ssim=0.9639]\u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:19<00:19,  6.62s/it, lpips=1.3851, psnr=24.40dB, ssim=0.9639]       \u001b[A\n",
      "Evaluating blur:  50%|█████     | 3/6 [00:28<00:19,  6.62s/it, lpips=1.5761, psnr=24.33dB, ssim=0.9631]\u001b[A\n",
      "Evaluating blur:  67%|██████▋   | 4/6 [00:28<00:14,  7.28s/it, lpips=1.5761, psnr=24.33dB, ssim=0.9631]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:28<00:14,  7.28s/it, lpips=1.5761, psnr=24.33dB, ssim=0.9631]\u001b[A\n",
      "Evaluating jpeg:  67%|██████▋   | 4/6 [00:34<00:14,  7.28s/it, lpips=1.3602, psnr=24.30dB, ssim=0.9638]\u001b[A\n",
      "Evaluating jpeg:  83%|████████▎ | 5/6 [00:34<00:07,  7.00s/it, lpips=1.3602, psnr=24.30dB, ssim=0.9638]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:34<00:07,  7.00s/it, lpips=1.3602, psnr=24.30dB, ssim=0.9638]\u001b[A\n",
      "Evaluating combined:  83%|████████▎ | 5/6 [00:41<00:07,  7.00s/it, lpips=1.5393, psnr=24.21dB, ssim=0.9617]\u001b[A\n",
      "Evaluating combined: 100%|██████████| 6/6 [00:41<00:00,  6.86s/it, lpips=1.5393, psnr=24.21dB, ssim=0.9617]\n",
      "Saving final model...: 100%|██████████| 5/5 [1:52:30<00:00, 1350.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      " mask: PSNR = 24.07dB, SSIM = 0.9617, LPIPS = 1.4006945230066776\n",
      " mask Inference Time: 0.36ms\n",
      " gaussian: PSNR = 24.45dB, SSIM = 0.9649, LPIPS = 1.34086924046278\n",
      " gaussian Inference Time: 0.28ms\n",
      " salt_pepper: PSNR = 24.40dB, SSIM = 0.9639, LPIPS = 1.385096449404955\n",
      " salt_pepper Inference Time: 0.26ms\n",
      " blur: PSNR = 24.33dB, SSIM = 0.9631, LPIPS = 1.576094999909401\n",
      " blur Inference Time: 0.25ms\n",
      " jpeg: PSNR = 24.30dB, SSIM = 0.9638, LPIPS = 1.3601593896746635\n",
      " jpeg Inference Time: 0.25ms\n",
      " combined: PSNR = 24.21dB, SSIM = 0.9617, LPIPS = 1.5392706990242004\n",
      " combined Inference Time: 0.24ms\n",
      "Metrics saved to /kaggle/working/metrics/final_metrics.json\n",
      "Final model saved to /kaggle/working/checkpoints/vae_final_model.pth\n",
      "\n",
      "ClearVision Pipeline completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final comparison image saved to /kaggle/working/final_results.png\n",
      "\n",
      "Model Testing Results:\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Mask Corruption:\n",
      "  PSNR: 24.13dB\n",
      "  SSIM: 0.9735\n",
      "  LPIPS: 1.4343\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Gaussian Corruption:\n",
      "  PSNR: 24.42dB\n",
      "  SSIM: 0.9676\n",
      "  LPIPS: 1.2685\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Salt_pepper Corruption:\n",
      "  PSNR: 24.63dB\n",
      "  SSIM: 0.9596\n",
      "  LPIPS: 1.6075\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Blur Corruption:\n",
      "  PSNR: 24.33dB\n",
      "  SSIM: 0.9628\n",
      "  LPIPS: 2.1067\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Jpeg Corruption:\n",
      "  PSNR: 24.03dB\n",
      "  SSIM: 0.9590\n",
      "  LPIPS: 1.4422\n",
      "Warning: LPIPS package not found. Using AlexNet features as approximation.\n",
      "For better results, install LPIPS with: pip install lpips\n",
      "Combined Corruption:\n",
      "  PSNR: 22.78dB\n",
      "  SSIM: 0.9537\n",
      "  LPIPS: 1.9756\n",
      "\n",
      "Test images saved to /kaggle/working/test_outputs\n",
      "\n",
      "    ## ClearVision Image Restoration Complete\n",
      "    \n",
      "    The model has been trained to restore corrupted images using a VAE architecture.\n",
      "    It can handle multiple corruption types:\n",
      "    - Masked regions (inpainting)\n",
      "    - Gaussian noise\n",
      "    - Salt and pepper noise\n",
      "    - Blur\n",
      "    - JPEG compression artifacts\n",
      "    - Combined corruptions\n",
      "    \n",
      "    To test on your own images, use:\n",
      "    ```\n",
      "    restore_image(model, 'path/to/your/image.jpg', corruption_type='combined')\n",
      "    ```\n",
      "    \n",
      "    Available corruption types: 'mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create necessary directories\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('samples', exist_ok=True)\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    # Run the main pipeline\n",
    "    model, train_loader, val_loader = main()\n",
    "    \n",
    "    # Test the model\n",
    "    test_model(model, val_loader)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    ## ClearVision Image Restoration Complete\n",
    "    \n",
    "    The model has been trained to restore corrupted images using a VAE architecture.\n",
    "    It can handle multiple corruption types:\n",
    "    - Masked regions (inpainting)\n",
    "    - Gaussian noise\n",
    "    - Salt and pepper noise\n",
    "    - Blur\n",
    "    - JPEG compression artifacts\n",
    "    - Combined corruptions\n",
    "    \n",
    "    To test on your own images, use:\n",
    "    ```\n",
    "    restore_image(model, 'path/to/your/image.jpg', corruption_type='combined')\n",
    "    ```\n",
    "    \n",
    "    Available corruption types: 'mask', 'gaussian', 'salt_pepper', 'blur', 'jpeg', 'combined'\n",
    "    \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1307206,
     "sourceId": 2177371,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6774.574675,
   "end_time": "2025-05-20T19:30:38.374167",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-20T17:37:43.799492",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
